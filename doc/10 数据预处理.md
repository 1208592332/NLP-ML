
# <center> 第 10 章 数据预处理 </center>
 
> 本章导读： 数据预处理的整个步骤流程在自然语言处理的工程中要比其在机器学习的工程中精简一些，最大的区别就在数据清洗和特征构造这两个至关重要的过程。在自然语言处理中特征构造是否良好，很大程度上取决于所构造的特征数据集的数据特性与文本内容语义吻合程度的高低。比如，文本情感分类和文本内容分类都属于分类范畴，但对于同一种算法（参数都调整到最优），在两个不同分类的业务下，得到的结果可能会相差很大。通过仔细分析，我们将不难发现造成这种差异的最根本原因就是构造出来的特征数据集的数据模式没有很好地契合文本的真实语义，这也是自然语言处理的最大难点所在。

## 10.1 数据清洗

在自然语言处理的过程中，通常我们得到的数据并非完全“干净”的，那么就需要将语言信息中的干扰信息除去，这样才能更有效去挖掘出其中包含的重要信息。文本内容形式多种多样，常见的包括：TXT文本、HTML文本、XML文本、word文档、excel文档等。在机器学习过程中，数据清洗的内容步骤是相当复杂的，但对文本来说，清洗的目的很明确简单：排除非关键信息。只需要保留文本内容所阐述的文字信息即可，尽可能减小这些信息对算法模型构建的影响。

以HTML文本为例，HTML文本中有很多HTML标签，如"body","title","p"标签等，毫无疑问将这些标签作为有效信息来训练模型是不可取的，因为它们与文本内容所要表达的主题无任何关联。那么清洗这些标签需要自己写很多代码吗？答案是：当然不用！因为在Java语言中，有Jsoup、HtmlParser、Apache tika、HtmlCleaner以及XPath等功能包，它们可以来帮助你完成对HTML文本的清洗工作；而在Python语言中，有BeautifulSoup、SGMLParser、HTMLParaer等功能包也能完成清洗工作，所以无论你是用Java语言，还是Python语言来进行你的自然语言处理工程，行业前辈们早已给后辈们种下了无数的“乘凉树”，你需要做的事就是好好“乘凉”！当然在实际情况中可能会遇到一些特殊的情况，或许这些功能包可能无法帮你很好的完成所有的清洗工作，但是不用担心，正则表达式会满足你的需求。
##	10.2 分词处理
在对文本进行清洗之后，需要给文本分词，这样能更容易挖掘出文本内容的一些特征。在上一章中提到了常用的一些分词工作包，如Stanford NLP分词、jieba分词、中科院NLPIR汉语分词，这些分词工作的分词效果都很不错，具体使用也很简单。比如有这样一句话：“自然语言处理（NLP）是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域！”，采用jieba分词后的效果：“自然语言处理/n （/x NLP/eng ）/n 是/v 计算机科学/n ，/x 人工智能/n ，/x 语言学/n 关注/v 计算机/n 和/c 人类/n （/x 自然/d ）/ x 语言/n 之间/f 的/uj 相互作用/l 的/uj 领域/n ！/x”，通常像一些停词和标点符号需要排除，如"是"、"和"、“的”、“，”、“（”、“）”等。但在一些某些业务情况下，有些停词和标点符号的确对业务的建模是有一定的帮助，比如对微博内容的情感判断。
##	10.3 特征构造
在做文本特征构造的时候，需要先了解向量空间模型（VSM：Vector Space Model）这一个概念。向量空间模型是把对文本内容的处理简化为向量空间中的向量运算，并且它以空间上的相似度表达语义的相似度，直观易懂。自然语言处理中，几乎所以的特征构造方法都是基于这个概念的。

 

1. 词袋模型
在传统的词袋模型当中，对于每一个词采用one-hot稀疏编码的形式，假设目标语料中共有N个唯一确认的词，那么需要一个长度N的词典，词典的每一个位置表达了文本中出现的某一个词。在某一种特征表达下，比如词频、binary、tf-idf等，可以将任意词，或者文本表达在一个N维的向量空间里。

2. N-gram模型
N-gram是一种统计语言模型，用来根据前(n-1)个item来预测第n个item，N-gram被广泛的应用于语音识、输入法、分词应等，当n分别为1、2、3时，又分别称为一元语法（unigram）、二元语法（bigram）与三元语法（trigram）。现已有一些学者用N-gram模型来构造分类任务的数据特征，本人也利用N-gram模型来构造了新闻突发事件判断的数据特征，并且判断的效果还挺不错。

3. 词向量
Word2vec是Google在2013年开源的将词表征为实数值向量的工具，利用深度学习训练，把对文本内容的处理简化为K维向量空间中的向量运算，而向量空间上的相似度可以用来表示文本语义上的相似度。word2vec核心是神经网络，并采用 CBOW（Continuous Bag-Of-Words，即连续的词袋模型）和 Skip-Gram 两种模型，将词语映像到同一坐标系。其实可以这样理解思考，单词为文本的表面特征，通过Word2vec就可以把表面特征映射到K维向量空间，挖掘出这些表面特征所对应的隐含特征，为文本数据寻求更加深层次的特征表示，在一定程度上可以达到揭示单词之间某些非线性关系的目的，也可以将其理解为词间的语义关系。一个典型的例子，用“国王”减去“男人”加上“女人”，能得到“王后”。

##	10.4 特征降维与选择
在数据特征构造完成后，通常需要对特征数据集进行特征降维和特征选择，两者的目标都是要使得数据集的特征维数减少，但两者又存在一定的区别。数据降维，一般说的是维数约简（Dimensionality reduction）。它的思路是：将原始高维特征空间里的点向一个低维空间投影，新的空间维度低于原特征空间，这样达到维数减少的目的。在这个过程中，特征发生了根本性的变化，原始的特征消失了（虽然新的特征也保持了原特征的一些性质）。而特征选择，是从 n 个特征中选择 d (d<n) 个出来，而其它的 n-d 个特征舍弃。所以，新的特征只是原来特征的一个子集。没有被舍弃的 d 个特征没有发生任何变化。

###	10.4.1 特征降维
当特征数据集构造完成后，可能会出现特征矩阵过大，从而会导致计算量大，训练时间长等一些列问题，因此降低特征矩阵维度也是必不可少的。机器学习中常见的特征降维方法：L1惩罚项的模型、主成分分析法（PCA）、线性判别分析（LDA）。PCA和LDA有很多的相似点，他们的共同原理是将原始样本映射到维度更低的样本空间中，PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。在自然语言处理中，常采用的方法就是主题模型，主题模型同时具备了降维和语义表达的效果，比如LSI、LDA、PLSA、HDP等统计主题模型，这些模型寻求文本在低维空间（不同主题上）的表达，在降低维度的同时，尽可能保留原有文本的语义信息，主题模型在处理中长度文本分类任务时特别有效。

###	10.4.2 特征选择
当特征数据集特征过多时，选择相对更有意义的特征来进行建模是很有必要的，去掉无关特征，保留相关特征的过程，也可以换句话说，从所有的特征中选择一个最好的特征子集。特征选择本质上可以认为是降维的过程。



1. Filter（过滤法）：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。如：方差选择法、相关系数法、卡方检验法、互信息法。
方差选择法：使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。
相关系数法：使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。
卡方检验法：经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量。
互信息法： 经典的互信息也是评价定性自变量对定性因变量的相关性的。



1. Wrapper（包装法）：根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。如：递归特征消除法。
递归特征消除法：递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。


1. Embedded（嵌入法）：先使用某些机器学习的算法模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。
基于惩罚项的特征选择法：使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型。如：基于惩罚项的特征选择法、基于树模型的特征选择法。

	基于树模型的特征选择法：树模型中（随机森林、GBDT和Xgboost）也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合树模型。


1. 深度学习方法：从深度学习模型中选择某一神经层的特征后也可以用来进行最终目标模型的训练。

##	10.5 简单实例
本章的上面几个小节依次描述了自然语言处理中的数据预处理需要的流程步骤，为了让大家能更清楚如何自己来进行实际处理，接下来用一个简单的实例场景来实战一下：
 
场景介绍：假设你在一家国内做资讯业务的新媒体公司做研发人员，你的直接领导交给你一个研发任务,希望你完成新闻自动分类这个功能，并拷贝了一份已分类的新闻资讯数据给你，这些数据是利用爬虫工具在互联网上的多个新闻网站上爬去下来的。这时，你该如何去完成这个任务喃？ 不用着急，按照本章的上面几个小节知识就可以基本帮你搞定这个任务。

> **（1） 数据清洗**

一般通过爬虫爬取到的文本都是包含html标签，我们需要把这些标签清洗掉，保证文本内容的正确性。例如，以下一段文本：
```<div><p>
<div class="video" mode="player" site="qiniu" src="https://dqdvideofile.qnssl.com/QYibRDG7_7101932251.mp4?sign=6fdec7415956db29a3f54a4fa5a82eb8&t=5abf5925" title="" thumb="https://o6yh618n9.qnssl.com/QYibRDG7_7101932251.mp4?vframe/jpg/offset/1"  hash="b0b0748feeb14182bd16da14155d0cad"></div>
</p><p>北京时间2013年4月23日英超第34轮的比赛中，曼联3-0大胜阿斯顿维拉提前4轮锁定球队历史第20个联赛冠军。比赛中范佩西上演了精彩的帽子戏法，可谓是居功至伟。下面就一起回顾一下这场经典的比赛吧。</p></div>```

清洗标签的方法很多，可以采用别人提供的功能包，也可以采用正则表达式去自定义处理。个人比较偏爱使用后者来清洗文本中的html标签，原因你可以去好好体会一下。利用正则表达式对html文本处理python代码如下：
<pre>
# 正则表达式过滤处理HTML中的标签
# @param htmlstr HTML字符串.
def filter_tags(htmlstr):
    # 先过滤CDATA
    re_cdata = re.compile('//<!CDATA\[[ >]∗ //\] > ',re.I) #匹配CDATA
    re_script = re.compile('<\s*script[^>]*>[^<]*<\s*/\s*script\s*>', re.I)
    # 过滤Script
    re_style = re.compile('<\s*style[^>]*>[^<]*<\s*/\s*style\s*>', re.I)
    # 过滤style
    re_br = re.compile('<br\s*?/?>')
    # 处理换行
    re_h = re.compile('</?\w+[^>]*>')
    # HTML标签
    re_comment = re.compile('<!--[^>]*-->')
    # HTML注释
    s = re_cdata.sub('', htmlstr)
    # 去掉CDATA
    s = re_script.sub('', s)  # 去掉SCRIPT
    s = re_style.sub('', s)
    # 去掉style
    s = re_br.sub('', s)
    # 将br转换为换行
    s = re_h.sub('', s)  # 去掉HTML 标签
    s = re_comment.sub('', s)
    # 去掉HTML注释
    # 去掉多余的空行
    blank_line = re.compile('\n+')
    s = blank_line.sub('', s)
    blank_line_l = re.compile('\n')
    s = blank_line_l.sub('', s)
    blank_kon = re.compile('\t')
    s = blank_kon.sub('', s)
    blank_one = re.compile('\r\n')
    s = blank_one.sub('', s)
    blank_two = re.compile('\r')
    s = blank_two.sub('', s)
    blank_three = re.compile(' ')
    s = blank_three.sub('', s)
    s = replaceCharEntity(s)  # 替换实体
    return s

# 使用正常的字符替换HTML中特殊的字符实体.你可以添加新的实体字符到CHAR_ENTITIES中,处理更多HTML字符实体.
# @param htmlstr HTML字符串.
def replaceCharEntity(htmlstr):
    CHAR_ENTITIES = {'nbsp': ' ', '160': ' ',
                     'lt': '<', '60': '<',
                     'gt': '>', '62': '>',
                     'amp': '&', '38': '&',
                     'quot': '"''"', '34': '"', }

    re_charEntity = re.compile(r'&#?(?P<name>\w+);')
    sz = re_charEntity.search(htmlstr)
    while sz:
        entity = sz.group()  # entity全称，如>
        key = sz.group('name')  # 去除&;后entity,如>为gt
        try:
            htmlstr = re_charEntity.sub(CHAR_ENTITIES[key], htmlstr, 1)
            sz = re_charEntity.search(htmlstr)
        except KeyError:
            # 以空串代替
            htmlstr = re_charEntity.sub('', htmlstr, 1)
            sz = re_charEntity.search(htmlstr)
    return htmlstr
</pre>
清洗过后，基本就是纯文本的文字描述了。

北京时间2013年4月23日英超第34轮的比赛中，曼联3-0大胜阿斯顿维拉提前4轮锁定球队历史第20个联赛冠军。比赛中范佩西上演了精彩的帽子戏法，可谓是居功至伟。下面就一起回顾一下这场经典的比赛吧。

> **（2） 分词处理**

分词工具很多，在利用python语言中常用的就是jieba分词，安装和使用也很简单，这里不再过多描述，来看看jieba的分词效果：

北京/ns 时间/n 2013/m 年/m 4/m 月/m 23/m 日/m 英超/nr 第/m 34/m 轮/q 的/uj 比赛/vn 中/f ，/x 曼联/ns 3/x -/x 0/x 大胜/nr 阿斯顿维拉/ns 提前/v 4/m 轮/n 锁定/v 球队/n 历史/n 第/m 20/m 个/m 联赛/vn 冠军/n 。/x 比赛/vn 中/f 范佩西/nr 上演/v 了/ul 精彩/n 的/uj 帽子戏法/n ，/x 可谓/v 是/v 居功至伟/nr 。/x 下面/f 就/d 一起/m 回顾/v 一下/m 这场/mq 经典/n 的/uj 比赛/vn 吧/y 。/x 

分词之后的格式为：单词/词性，通常在某些业务建模情况下，词性可能用不上，但是词性对分析特征提供了参考。例如，要判断这篇文章的情感色彩，那么根据词性就可以将“这场”，“下面”等干扰词排除。
分析分词之后的数据不难发现，有些词和标点符号其实在文本中都是很常见的，其实用专业的一点术语来说，这些词和标点符号多分析处理问题没有多大用处或根本没有作用，可能甚至会产生一定的负面干扰。这不需要什么公司定理来验证说明，经验就告诉我们这些词和标点应该剔除，所以就有了停用词和去除停用词的说法。当然，为了尽可能的排除干扰数据，通常在分词之后都要再进行停用词处理，处理后的结果如下：

北京/ns 时间/n 英超/nr 比赛/vn 曼联/ns 大胜/nr 阿斯顿维拉/ns 提前/v 锁定/v 球队/n 历史/n 联赛/vn 冠军/n 比赛/vn 范佩西/nr 上演/v 精彩/n 帽子戏法/n 可谓/v 居功至伟/nr 下面/f 一起/m 回顾/v 一下/m 这场/mq 经典/n 比赛/vn 

> **（3） 特征构造**

分词并进行停用词处理之后，就需要对数据进行特征构造，也可以说成特征转换。在处理文本数据的时候，基于向量空间模型这一概念，可以通过词袋模型、N-gram模型、词向量来进行特征构造。例如，通过求每一个单词的tf-idf值，就可以将用tf-idf值来代替单词，将文本转换成一个向量。具体步骤如下：

1. 将所有分词后的文本，按单词为最小单位去重，构成一个词汇表。
1. 计算词汇表中的每一个单词的tf-idf值，可以将tf-idf值较小词直接剔除，这样可以减少向量的维度。
1. 用词汇表将每一个文本转化成维度一样的向量，并且非零值即为单词所对应的tf-idf值。

除此之外，也可以用N-gram模型和词向量来讲单词转化成对应的数值向量。但是，实验证明针对新闻分类业务tf-idf和词向量方法更适合，两者的区别在于：tf-idf方法是直接值替换单词，而词向量方法需要构建一个词向量模型，通过模型将单词映射在多个维度上，简单来说，就是用一个n维向量来表示一个单词。比如，“曼联”这个词，tf-idf值为0.113，那么其在整个文本中被表示为0.113；而词向量方法可能会使“曼联”这个词被表示为（0.103，-0.011，0.232，...,0.145）。值得一提的是：针对新闻分类问题可以直接采用基于概率分布的主题模型来完成文本的特征构造，这也是常用且效果很好的方法，由于主题模型涉及的知识点比较多，这里就不再细说了。

> **（4） 特征降维与选择**
在文本的数据特征构造完成以后，可能一篇文本可能会被表示成一个成百上千维的向量，通常情况下会出现数据集稀疏的情况，那么这时就需要对数据集进行降维或选择。方法很多，基于模型的特征降维或特征选择效果通常会更好。例如，上文中领导提供的数据，通过清洗、分词、特征构造之后，形成了一个(M,N)的特征数据集，采用随机森林算法可以将特征数据集将到（m，N）,其中m<<M。这样就可以直接将这个(m，N)的特征数据集来构建分类模型了。

##	10.6 本章小结
数据预处理的整个步骤流程在自然语言处理的工程中要比机器学习的工程中精简一些，最大的区别就是数据清洗和特征构造，这两个过程也是十分至关重要。而且在自然语言处理中特征构造是否良好，很大程度取决于所构造的特征数据集的数据特性与文本内容语义吻合程度的高低。比如，文本情感分类和文本内容分类都是属于分类范畴，但是对于同一种算法（参数都调整到最优），在两个不同分类的业务下，得到的结果可能会相差很大，通过仔细分析，不难发现造成这种差异其实最根本的原因就是构造出来的特征数据集的数据模式没有很好的契合文本的真实语义，这也是自然语言处理的最大难点之处。

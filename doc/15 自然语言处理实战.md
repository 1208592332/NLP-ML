# <center> 第 15 章 自然语言处理实战 </center>


> 本章导读：
## 15.1 GitHub数据提取与可视化分析

GitHub作为全球最大的代码托管平台，每小时都有成千上万个项目产生，他为开源作出了不可磨灭的贡献。本文使用了NetworkX对GitHub的进行图形分析，通过gitHub的丰富数据，构建可以在各种不同的方式使用数据模型。这里将github用户、代码、仓库构建成兴趣图。本文包含三方面:github 开发者平台和对应的api、如何使用NetworkX作图、构建github的兴趣图和图算法

### 15.1.1 了解github的API
同Twitter和Facebook一样，第一步就是获取Git自带的API。地址：https://developer.github.com/v3/
其中大部分我们并不需要，我们仅仅关注的是用户和仓库。所有的API访问通过HTTPS，并从https://api.github.com访问。所有发送和接收的数据是JSON。
> 创建API连接

github实现了OAuth接口，在拥有github账户后获取API通道的方式有两种。一种叫做personal access token.你可以为你自己的使用或实现的Web流创建一个个人访问令牌，以允许其他用户授权你的应用程序。一种是OAuth application，所有的开发人员在开始之前都需要注册他们的应用程序。注册OAuth应用分配一个唯一的客户ID和客户的密钥。这里简单起见，采用personal access token。点击新建即可生成有对应权限的token。生成好的token如下所示：
![](https://i.imgur.com/kg5w8Ch.png)

向api根目录发送请求，试一试这个token连接情况：

<pre>
curl https://api.github.com/?access_token=$TOKEN
{
  "current_user_url": "https://api.github.com/user",
  "current_user_authorizations_html_url": "https://github.com/settings/connections/applications{/client_id}",
  "authorizations_url": "https://api.github.com/authorizations",
  "code_search_url": "https://api.github.com/search/code?q={query}{&page,per_page,sort,order}",
  "emails_url": "https://api.github.com/user/emails",
  "emojis_url": "https://api.github.com/emojis",
  "events_url": "https://api.github.com/events",
  "feeds_url": "https://api.github.com/feeds",
  "followers_url": "https://api.github.com/user/followers",
  "following_url": "https://api.github.com/user/following{/target}",
  "gists_url": "https://api.github.com/gists{/gist_id}",
  "hub_url": "https://api.github.com/hub",
  "issue_search_url": "https://api.github.com/search/issues?q={query}{&page,per_page,sort,order}",
  "issues_url": "https://api.github.com/issues",
  "keys_url": "https://api.github.com/user/keys",
  "notifications_url": "https://api.github.com/notifications",
  "organization_repositories_url": "https://api.github.com/orgs/{org}/repos{?type,page,per_page,sort}",
  "organization_url": "https://api.github.com/orgs/{org}",
  "public_gists_url": "https://api.github.com/gists/public",
  "rate_limit_url": "https://api.github.com/rate_limit",
  "repository_url": "https://api.github.com/repos/{owner}/{repo}",
  "repository_search_url": "https://api.github.com/search/repositories?q={query}{&page,per_page,sort,order}",
  "current_user_repositories_url": "https://api.github.com/user/repos{?type,page,per_page,sort}",
  "starred_url": "https://api.github.com/user/starred{/owner}{/repo}",
  "starred_gists_url": "https://api.github.com/gists/starred",
  "team_url": "https://api.github.com/teams",
  "user_url": "https://api.github.com/users/{user}",
  "user_organizations_url": "https://api.github.com/user/orgs",
  "user_repositories_url": "https://api.github.com/users/{user}/repos{?type,page,per_page,sort}",
  "user_search_url": "https://api.github.com/search/users?q={query}{&page,per_page,sort,order}"
}
</pre>

github 的API符合HATEOAS设计，从上面可以看到，如果想获取当前用户的信息，应该去访问api.github.com/user，然后就得到了下面结果。

<pre>
{
"login": "luzhijun",
"id": 15256911,
"avatar_url": "https://avatars.githubusercontent.com/u/15256911?v=3",
"gravatar_id": "",
"url": "https://api.github.com/users/luzhijun",
"html_url": "https://github.com/luzhijun",
...
}
</pre>

> pygithub

ok，这里介绍一个方便python使用的包，pygithub,你可以通过它方便地用python脚本管理github。其API与github对应。 举个例子，拿到指定用户的所有仓库:

<pre>
from github import Github
# Specify your own access token here
ACCESS_TOKEN = ''
USER = 'luzhijun'
client = Github(ACCESS_TOKEN)
user = client.get_user(USER)
REPOS=user.get_repos()
print(list(REPOS))
</pre>

> [Repository(full_name=”luzhijun/huxblog-boilerplate”), Repository(full_name=”luzhijun/leecode”), Repository(full_name=”luzhijun/luzhijun.github.io”), Repository(full_name=”luzhijun/Optimization”), Repository(full_name=”luzhijun/SVDRecommenderSystem”)]
###	15.1.2 如何使用NetworkX作图
> 了解NetworkX

NetworkX是一个用Python语言开发的图论与复杂网络建模工具,内置了常用的图与复杂网络分析算法,可以方便的进行复杂网络数据分析、仿真建模等工作。
下面创建一个有向图x->y:
<pre>
import networkx as nx

# 创建有向图
g = nx.DiGraph()
# 加条边x->y
g.add_edge('X', 'Y')
# 打印图的相关数据信息
print (nx.info(g),'\n')

print ("Nodes:", g.nodes())
print ("Edges:", g.edges())
# 节点属性
print ("X props:", g.node['X'])
print ("Y props:", g.node['Y'])
# 边属性
print ("X=>Y props:", g['X']['Y'])
# 更新节点信息
g.node['X'].update({'prop1' : 'value1'})
print ("X props:", g.node['X'])
# 更新边信息
g['X']['Y'].update({'label' : 'label1'})
print ("X=>Y props:", g['X']['Y'])
</pre>

	Name: 
	Type: DiGraph #无向图表示为Graph Number of nodes: 2
	Number of edges: 1
	Average in degree: 0.5000
	Average out degree: 0.5000
	
	Nodes: [‘Y’, ‘X’]
	Edges: [(‘X’, ‘Y’)]
	X props: {}
	Y props: {}
	X=>Y props: {}
	X props: {‘prop1’: ‘value1’}
	X=>Y props: {‘label’: ‘label1’}

有向图和无向图都可以给边赋予权重，用到的方法是add_weighted_edges_from，它接受1个或多个三元组[u,v,w]作为参数，其中u是起点，v是终点，w是权重。例如：
<pre>
g.add_weighted_edges_from([('X','Y',10.0)])
print (g.get_edge_data('X','Y'))
</pre>
	{‘weight’: 10.0, ‘label’: ‘label1’}

NetworkX提供了常用的图论经典算法，例如DFS、BFS、最短路、最小生成树、最大流等等，非常丰富，如果不做复杂网络，只作图论方面的工作，也可以应用NetworkX作为基本的开发包，更多用法参照networkx。

###	15.1.3 使用NetworkX构建兴趣图

兴趣图和社交网络图是有区别的，最大的不同就是兴趣图节点代表的不一定是同一类的东西。有个项目是我加星的，那么这个兴趣图就是：
我–(gazes)–>项目
那么还有其他很多人也关注了这个项目，我们把加星的人都找出来。
<pre>
from github import Github
import networkx as nx

ACCESS_TOKEN = ''
USER='minrk'
REPO='findspark'
client = Github(ACCESS_TOKEN)
user = client.get_user(USER)
repo=user.get_repo(REPO)
stargazers=list(repo.get_stargazers())#加星的用户集合

g = nx.DiGraph()
g.add_node(repo.name + '(repo)', type='repo', lang=repo.language, owner=user.login)

for sg in stargazers:
    g.add_node(sg.login + '(user)', type='user')
    g.add_edge(sg.login + '(user)', repo.name + '(repo)', type='gazes')
# 打印图的基本属性   
print (nx.info(g),'\n')
# 打印项目和用户点的基本属性
print (g.node['findspark(repo)'])
print (g.node['luzhijun(user)'],'\n')
# 打印这条边属性
print (g['luzhijun(user)']['findspark(repo)'])
# 打印起点为XXX的信息
print (g['luzhijun(user)'])
print (g['findspark(repo)'])
# 打印用户的出入度信息
print (g.in_edges(['luzhijun(user)']))
print (g.out_edges(['luzhijun(user)']))
# 打印项目的出入度信息
print (g.in_edges(['findspark(repo)']))
print (g.out_edges(['findspark(repo)']))
</pre>

	Name: 
	Type: DiGraph
	Number of nodes: 81
	Number of edges: 80
	Average in degree: 0.9877 # 80/81 Average out degree: 0.9877 # 80/81
	
	{‘lang’: ‘Python’, ‘type’: ‘repo’, ‘owner’: ‘minrk’}
	{‘type’: ‘user’}
	
	{‘type’: ‘gazes’}
	{‘findspark(repo)’: {‘type’: ‘gazes’}}
	{}
	[]
	[(‘luzhijun(user)’, ‘findspark(repo)’)]
	[(‘lendenmc(user)’, ‘findspark(repo)’), (‘nehalecky(user)’, ‘findspark(repo)’), (‘charsmith(user)’, ‘findspark(repo)’), (‘cwharland(user)’, ‘findspark(repo)’), (‘d3borah(user)’, ‘findspark(repo)’), (‘cimox(user)’, ‘findspark(repo)’), (‘dirmeier(user)’, ‘findspark(repo)’), (‘paulochf(user)’, ‘findspark(repo)’), (‘qingniufly(user)’, ‘findspark(repo)’), (‘hmourit(user)’, ‘findspark(repo)’), (‘ryan-williams(user)’, ‘findspark(repo)’), (‘rholder(user)’, ‘findspark(repo)’), (‘xysmas(user)’, ‘findspark(repo)’), (‘linkTDP(user)’, ‘findspark(repo)’), (‘minimaxir(user)’, ‘findspark(repo)’), (‘KLXN(user)’, ‘findspark(repo)’), (‘quadnix(user)’, ‘findspark(repo)’), (‘luzhijun(user)’, ‘findspark(repo)’), (‘chrinide(user)’, ‘findspark(repo)’), (‘jaredmichaelsmith(user)’, ‘findspark(repo)’), (‘rgbkrk(user)’, ‘findspark(repo)’), (‘jiamo(user)’, ‘findspark(repo)’), (‘drizham(user)’, ‘findspark(repo)’), (‘assumednormal(user)’, ‘findspark(repo)’), (‘cvincent00(user)’, ‘findspark(repo)’), (‘xiaohan2012(user)’, ‘findspark(repo)’), (‘dapurv5(user)’, ‘findspark(repo)’), (‘pchalasani(user)’, ‘findspark(repo)’), (‘benetka(user)’, ‘findspark(repo)’), (‘rohithreddy(user)’, ‘findspark(repo)’), (‘seanjensengrey(user)’, ‘findspark(repo)’), (‘opikalo(user)’, ‘findspark(repo)’), (‘amontalenti(user)’, ‘findspark(repo)’), (‘Bekterra(user)’, ‘findspark(repo)’), (‘Grillz(user)’, ‘findspark(repo)’), (‘nchammas(user)’, ‘findspark(repo)’), (‘markbarks(user)’, ‘findspark(repo)’), (‘jesusjsc(user)’, ‘findspark(repo)’), (‘d2207197(user)’, ‘findspark(repo)’), (‘Erstwild(user)’, ‘findspark(repo)’), (‘henridf(user)’, ‘findspark(repo)’), (‘ranjankumar-gh(user)’, ‘findspark(repo)’), (‘locojay(user)’, ‘findspark(repo)’), (‘WilliamQLiu(user)’, ‘findspark(repo)’), (‘szinya(user)’, ‘findspark(repo)’), (‘seanjh(user)’, ‘findspark(repo)’), (‘vherasme(user)’, ‘findspark(repo)’), (‘stared(user)’, ‘findspark(repo)’), (‘freeman-lab(user)’, ‘findspark(repo)’), (‘wy36101299(user)’, ‘findspark(repo)’), (‘esafak(user)’, ‘findspark(repo)’), (‘napjon(user)’, ‘findspark(repo)’), (‘richardskim111(user)’, ‘findspark(repo)’), (‘SimonArnu(user)’, ‘findspark(repo)’), (‘lmillefiori(user)’, ‘findspark(repo)’), (‘binhe22(user)’, ‘findspark(repo)’), (‘robcowie(user)’, ‘findspark(repo)’), (‘jazzwang(user)’, ‘findspark(repo)’), (‘Dumbris(user)’, ‘findspark(repo)’), (‘giulioungaretti(user)’, ‘findspark(repo)’), (‘cbouey(user)’, ‘findspark(repo)’), (‘andrewiiird(user)’, ‘findspark(repo)’), (‘DaniGate(user)’, ‘findspark(repo)’), (‘rdhyee(user)’, ‘findspark(repo)’), (‘lgautier(user)’, ‘findspark(repo)’), (‘sandysnunes(user)’, ‘findspark(repo)’), (‘willcline(user)’, ‘findspark(repo)’), (‘aliciatb(user)’, ‘findspark(repo)’), (‘d18s(user)’, ‘findspark(repo)’), (‘he0x(user)’, ‘findspark(repo)’), (‘liulixiang1988(user)’, ‘findspark(repo)’), (‘alexandercbooth(user)’, ‘findspark(repo)’), (‘branning(user)’, ‘findspark(repo)’), (‘bguOIQ(user)’, ‘findspark(repo)’), (‘coder-chenzhi(user)’, ‘findspark(repo)’), (‘mdbishop(user)’, ‘findspark(repo)’), (‘Hguimaraes(user)’, ‘findspark(repo)’), (‘ocanbascil(user)’, ‘findspark(repo)’), (‘fish2000(user)’, ‘findspark(repo)’), (‘stzonis(user)’, ‘findspark(repo)’)]
	[]


###	15.1.4 NetWorkX部分统计指标

(1)度中心性（Degree Centrality):是在网络分析中刻画节点中心性（Centrality）的最直接度量指标。一个节点的节点度越大就意味着这个节点的度中心性越高，该节点在网络中就越重要。某点的度中心性[Math Processing Error]Cd(i)=d(i)n−1，范围为[0,1]。

- degree_centrality(G) Compute the degree centrality for nodes.
- in_degree_centrality(G) Compute the in-degree centrality for nodes.
- out_degree_centrality(G) Compute the out-degree centrality for nodes.

(2)中介中心性/中间中心性(Between Centrality) 。以经过某个节点的最短路径数目来刻画节点重要性的指标。如果两个不相邻的参与者k和j想要与对方互动而参与者i处在它们的路径上，那么i可能对它们之间的互动拥有一定的控制力。中介性用来度量i对于其他结点的控制能力。即如果i处在非常多结点的交互路径上，那么i就是一个重要的参与者。

- betweenness_centrality(G[, normalized, …]) Compute betweenness centrality for nodes.
- edge_betweenness_centrality(G[, normalized, …]) Compute betweenness centrality for edges.

(3)接近中心性（Closeness Centrality）反映在网络中某一节点与其他节点之间的接近程度。这种中心性的观察视角主要基于接近度或者距离。它的基本思想是如果一个参与者能很容易的与所有其他参与者进行互动，那么它就是中心的。即它到其他所以参与者的距离要足够短。于是，我们就可以使用最短距离来计算这个数值。假设参与者i和参与者j之间的最短距离记为d(i,j)（由最短路径上的链接数目度量）。Cc(i)=n−1∑j=1nd(i,j)

- closeness_centrality(G[, v, weighted_edges]) Compute closeness centrality for nodes.

作为学习，我们使用Krackhardt kite graph来研究，之所以称之为风筝图，因为长得像风筝。。。(废话)
![](https://i.imgur.com/lWxUV4Q.png)

<pre>
from operator import itemgetter
kkg = nx.generators.small.krackhardt_kite_graph()
print ("Degree Centrality")
print (sorted(nx.degree_centrality(kkg).items(), 
             key=itemgetter(1), reverse=True),'\n')
print ("Betweenness Centrality")
print (sorted(nx.betweenness_centrality(kkg).items(), 
             key=itemgetter(1), reverse=True),'\n')
print ("Closeness Centrality")
print (sorted(nx.closeness_centrality(kkg).items(), 
             key=itemgetter(1), reverse=True))
</pre>

	Degree Centrality [(3, 0.6666666666666666), (5, 0.5555555555555556), (6, 0.5555555555555556), (0, 0.4444444444444444), (1, 0.4444444444444444), (2, 0.3333333333333333), (4, 0.3333333333333333), (7, 0.3333333333333333), (8, 0.2222222222222222), (9, 0.1111111111111111)]
	
	Betweenness Centrality [(7, 0.38888888888888884), (5, 0.23148148148148148), (6, 0.23148148148148148), (8, 0.2222222222222222), (3, 0.10185185185185183), (0, 0.023148148148148143), (1, 0.023148148148148143), (2, 0.0), (4, 0.0), (9, 0.0)]
	
	Closeness Centrality [(5, 0.6428571428571429), (6, 0.6428571428571429), (3, 0.6), (7, 0.6), (0, 0.5294117647058824), (1, 0.5294117647058824), (2, 0.5), (4, 0.5), (8, 0.42857142857142855), (9, 0.3103448275862069)]
###	15.1.5 构建github的兴趣图
> 找出大神

大神一般都是关注者很高，在社区中很活跃，除了前面加星，类似微博、facebook，github我们还有“关注”关系可以用，现在对前面扩展关注关系，对每个加星的用户扩展。这里简单起见，关注者都是现有的结点，不再新增结点。 
由于API每小时限制使用5000个请求，打印下剩余请求数看有没有耗尽，耗尽的话抛出异常。

<pre>
import sys
for i, sg in enumerate(stargazers):   
    # 增加关注联系，如果有关注者的话
        try:
        for follower in sg.get_followers():
            if follower.login + '(user)' in g:
                g.add_edge(follower.login + '(user)', sg.login + '(user)', 
                           type='follows')
    except Exception: #ssl.SSLError
        sys.stderr.write( "Encountered an error fetching followers for", \
                             sg.login, "Skipping.")

    print ("Processed", i+1, " stargazers. Num nodes/edges in graph", \
          g.number_of_nodes(), "/", g.number_of_edges())
    print ("Rate limit remaining", client.rate_limiting)
</pre>

	rocessed 1 stargazers. Num nodes/edges in graph 81 / 80
	Rate limit remaining (4999, 5000)
	Processed 2 stargazers. Num nodes/edges in graph 81 / 83
	Rate limit remaining (4987, 5000)
	Processed 3 stargazers. Num nodes/edges in graph 81 / 84
	Rate limit remaining (4985, 5000)
	Processed 4 stargazers. Num nodes/edges in graph 81 / 85
	Rate limit remaining (4983, 5000)
	Processed 5 stargazers. Num nodes/edges in graph 81 / 86
	Rate limit remaining (4981, 5000)
	Processed 6 stargazers. Num nodes/edges in graph 81 / 90
	Rate limit remaining (4968, 5000)
	…
	Rate limit remaining (4857, 5000)
	Processed 78 stargazers. Num nodes/edges in graph 81 / 95
	Rate limit remaining (4856, 5000)
	Processed 79 stargazers. Num nodes/edges in graph 81 / 95
	Rate limit remaining (4855, 5000)
	Processed 80 stargazers. Num nodes/edges in graph 81 / 95
	Rate limit remaining (4854, 5000)

接下来，对关联关系统计。

<pre>
from operator import itemgetter
from collections import Counter

# 看下更新的图信息
print (nx.info(g),'\n')

# 每个打星用户的关注者数量不同
print (len([e for e in g.edges_iter(data=True) if e[2]['type'] == 'follows']),'\n')

# 查看某个打星用户有多少关注者
print (len([e 
           for e in g.edges_iter(data=True) 
               if e[2]['type'] == 'follows' and e[1] == 'freeman-lab(user)']),'\n')


# 打印度最多的前10个结点
print (list(sorted([n for n in g.degree_iter()], key=itemgetter(1), reverse=True)[:10]),'\n')


# 对每个打星用户的关注者数目计数
c = Counter([e[1] for e in g.edges_iter(data=True) if e[2]['type'] == 'follows'])
popular_users = [ (u, f) for (u, f) in c.most_common() if f > 0 ]
print ("Number of popular users", len(popular_users))
print ("Top popular users:", popular_users[:10])
</pre>

	Name: Type: DiGraph Number of nodes: 81 Number of edges: 95 Average in degree: 1.1728 Average out degree: 1.1728
	
	15
	
	4
	
	[(‘findspark(repo)’, 80), (‘rgbkrk(user)’, 6), (‘freeman-lab(user)’, 5), (‘esafak(user)’, 4), (‘andrewiiird(user)’, 4), (‘minimaxir(user)’, 3), (‘nchammas(user)’, 3), (‘rholder(user)’, 2), (‘chrinide(user)’, 2), (‘dapurv5(user)’, 2)]
	
	Number of popular users 9 Top popular users: [(‘freeman-lab(user)’, 4), (‘rgbkrk(user)’, 3), (‘minimaxir(user)’, 2), (‘rholder(user)’, 1), (‘amontalenti(user)’, 1), (‘rdhyee(user)’, 1), (‘stared(user)’, 1), (‘esafak(user)’, 1), (‘nchammas(user)’, 1)]

(‘freeman-lab(user)’, 4)指freeman-lab在findspark项目中的加星者中还有4个关注者，为了更清楚挖掘图信息，用上面介绍的几个图算法来看一下这个图，由于’仓库’结点有大量的度，其他结点相比来说度都很小，因此先删掉’仓库’结点再计算。
<pre>
from operator import itemgetter
h = g.copy()
# 移除中心结点
h.remove_node('findspark(repo)')

dc = sorted(nx.degree_centrality(h).items(), 
            key=itemgetter(1), reverse=True)

print ("Degree Centrality")
print (dc[:10],'\n')
bc = sorted(nx.betweenness_centrality(h).items(), 
            key=itemgetter(1), reverse=True)

print ("Betweenness Centrality")
print (bc[:10],'\n')

print ("Closeness Centrality")
cc = sorted(nx.closeness_centrality(h).items(), 
            key=itemgetter(1), reverse=True)
print (cc[:10])
</pre>

	Degree Centrality
	[(‘rgbkrk(user)’, 0.06329113924050633), 
	(‘freeman-lab(user)’, 0.05063291139240506),
	(‘esafak(user)’, 0.0379746835443038), 
	(‘andrewiiird(user)’, 0.0379746835443038), 
	(‘minimaxir(user)’, 0.02531645569620253), 
	(‘nchammas(user)’, 0.02531645569620253), 
	(‘rholder(user)’, 0.012658227848101266), 
	(‘chrinide(user)’, 0.012658227848101266), 
	(‘dapurv5(user)’, 0.012658227848101266), 
	(‘amontalenti(user)’, 0.012658227848101266)]
	
	Betweenness Centrality
	[(‘rgbkrk(user)’, 0.0009737098344693282), 
	(‘esafak(user)’, 0.0006491398896462187), 
	(‘nchammas(user)’, 0.0001622849724115547), 
	(‘linkTDP(user)’, 0.0), 
	(‘nehalecky(user)’, 0.0), 
	(‘charsmith(user)’, 0.0), 
	(‘cwharland(user)’, 0.0), 
	(‘d3borah(user)’, 0.0), 
	(‘cimox(user)’, 0.0), 
	(‘dirmeier(user)’, 0.0)]
	
	Closeness Centrality
	[(‘andrewiiird(user)’, 0.04050632911392405), 
	(‘esafak(user)’, 0.03375527426160337), 
	(‘chrinide(user)’, 0.028768699654775604), 
	(‘rgbkrk(user)’, 0.02531645569620253), 
	(‘alexandercbooth(user)’, 0.02278481012658228), 
	(‘dapurv5(user)’, 0.012658227848101266), 
	(‘Bekterra(user)’, 0.012658227848101266), 
	(‘nchammas(user)’, 0.012658227848101266), 
	(‘he0x(user)’, 0.012658227848101266), 
	(‘Hguimaraes(user)’, 0.012658227848101266)]

来，我们来见见两位大神，一位是数据显示度中心性和中介中心性最大的rgbkrk。

![](https://i.imgur.com/hd5TVM6.png)

Kyle Kelley来自硅谷，是jupyter、ipython、cloudpipe等大项目的开发组员，影响力很大。
另一位是接近中心性最大的andrewiiird。
![](https://i.imgur.com/gfRGk8Q.png)
andrew来自香港，关注与被关注比率几乎有两个数量级的差别，而Kyle Kelley这个比值接近一比一，也正说明了andrew的“互动性”很强的。(大部分时间关注别人而自己隐藏在暗处，😜)

> 找出活跃项目

现在图中只有一个项目，其余结点都是用户。为了找出活跃项目，对每个用户，遍历找其加星的项目添加到图中，扩充图的项目结点。
<pre>
MAX_REPOS = 500
for i, sg in enumerate(stargazers):
    print (sg.login)
    try:
        for starred in sg.get_starred()[:MAX_REPOS]: # Slice to avoid supernodes
            g.add_node(starred.name + '(repo)', type='repo', lang=starred.language, \
                       owner=starred.owner.login)
            g.add_edge(sg.login + '(user)', starred.name + '(repo)', type='gazes')
    except Exception: #ssl.SSLError:
        print ("Encountered an error fetching starred repos for", sg.login, "Skipping.")

    print ("Processed", i+1, "stargazers' starred repos")
    print ("Num nodes/edges in graph", g.number_of_nodes(), "/", g.number_of_edges())
    print ("Rate limit", client.rate_limiting)

</pre>

	pchalasani
	Processed 1 stargazers’ starred repos
	Num nodes/edges in graph 176 / 190
	Rate limit (4996, 5000)
	rgbkrk
	Encountered an error fetching starred repos for rgbkrk Skipping.
	Processed 2 stargazers’ starred repos
	Num nodes/edges in graph 266 / 280
	Rate limit (4993, 5000)
	napjon
	Processed 79 stargazers’ starred repos
	Num nodes/edges in graph 12692 / 17579
	Rate limit (4369, 5000)
	luzhijun
	Processed 80 stargazers’ starred repos
	Num nodes/edges in graph 12693 / 17581
	Rate limit (4368, 5000)

以上操作挺费时间的，结点数由几百个一下子扩充到上万个，可以把MAX_REPOS改小一点，然后去泡杯咖啡吧😁。
拿到所有项目数据，接下来就是要统计哪个是热点项目，可以查询哪个用户mark了哪些项目，他喜欢用啥程序语言，还有那些加星加到手软的用户也可以查询出来。
<pre>
print (nx.info(g),'\n')
# 拿到图里面的所有项目构成列表
repos = [n for n in g.nodes_iter() if g.node[n]['type'] == 'repo']

# 关注最多的前10个项目
print ("Popular repositories")
print (sorted([(n,d) 
              for (n,d) in g.in_degree_iter() 
                  if g.node[n]['type'] == 'repo'], \
             key=itemgetter(1), reverse=True)[:10])

print "Respositories that luzhijun has bookmarked"
print ([(n,g.node[n]['lang']) 
       for n in g['luzhijun(user)'] 
           if g['luzhijun(user)'][n]['type'] == 'gazes'])

# 用户喜爱的程序语言

print ("Programming languages luzhijun is interested in")
print (list(set([g.node[n]['lang'] 
                for n in g['luzhijun(user)'] 
                    if g['luzhijun(user)'][n]['type'] == 'gazes'])))
#查看关注项目最多的用户(超过MAX_REPOS)
print ("Supernode candidates")
print (sorted([(n, len(g.out_edges(n))) 
              for n in g.nodes_iter() 
                  if g.node[n]['type'] == 'user' and len(g.out_edges(n)) > MAX_REPOS], \
             key=itemgetter(1), reverse=True))
</pre>
	
	Name: 
	Type: DiGraph
	Number of nodes: 12693
	Number of edges: 17581
	Average in degree: 1.3851
	Average out degree: 1.3851
	
	Popular repositories
	[(‘findspark(repo)’, 80), (‘spark(repo)’, 27), (‘tensorflow(repo)’, 24), (‘luigi(repo)’, 21), (‘ipython(repo)’, 21), (‘data-science-ipython-notebooks(repo)’, 20), (‘awesome-public-datasets(repo)’, 20), (‘spark-notebook(repo)’, 19), (‘docker(repo)’, 18), (‘Probabilistic-Programming-and-Bayesian-Methods-for-Hackers(repo)’, 17)]
	
	Respositories that ocanbascil has bookmarked
	[(‘til(repo)’, ‘VimL’), (‘react-joyride(repo)’, ‘JavaScript’), (‘django(repo)’, ‘Python’), (‘peewee(repo)’, ‘Python’), (‘select2(repo)’, ‘JavaScript’), (‘requests(repo)’, ‘Python’), (‘benfords-law(repo)’, ‘JavaScript’), (‘daterangepicker(repo)’, ‘JavaScript’), (‘django-zappa(repo)’, ‘Python’), (‘layered(repo)’, ‘Python’), (‘coffeescript(repo)’, ‘CoffeeScript’), (‘awesome-jekyll(repo)’, None), (‘lektor(repo)’, ‘Python’), (‘aetycoon(repo)’, ‘Python’), (‘computer-science(repo)’, None), (‘PerformanceEngine(repo)’, ‘Python’), (‘pattern_classification(repo)’, ‘Jupyter Notebook’), (‘node-v0.x-archive(repo)’, None), (‘django-crispy-forms(repo)’, ‘Python’), (‘hug(repo)’, ‘Python’), (‘euler-coffeescript(repo)’, ‘CoffeeScript’), (‘pandas(repo)’, ‘Python’), (‘pachyderm(repo)’, ‘Go’), (‘eulerclash(repo)’, None), (‘Pipe(repo)’, ‘Python’), (‘data-science-ipython-notebooks(repo)’, ‘Python’), (‘TweetHit(repo)’, ‘Python’), (‘captionmash(repo)’, None), (‘django-tinymce(repo)’, ‘Python’), (‘dataset(repo)’, ‘Python’), (‘shepherd(repo)’, ‘CSS’), (‘data-science-from-scratch(repo)’, ‘Python’), (‘spark(repo)’, ‘Scala’), (‘data-science-blogs(repo)’, ‘Python’), (‘specter(repo)’, ‘Clojure’), (‘tensorflow(repo)’, ‘R’), (‘findspark(repo)’, ‘Python’), (‘localtunnel(repo)’, ‘JavaScript’)]
	
	Programming languages ocanbascil is interested in
	[‘CSS’, ‘VimL’, ‘Python’, ‘JavaScript’, ‘Clojure’, None, ‘CoffeeScript’, ‘R’, ‘Jupyter Notebook’, ‘Scala’, ‘Go’]
	
	Supernode candidates [(‘he0x(user)’, 501), (‘esafak(user)’, 501)]

从结果可看出，关注findspark项目的人大约三分之一也关注了spark和tensorflow，TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统，机器学习果真这么火热。大约四分之一的人关注了Luigi，查了下其主要目的是为了解决需要长期运行的流式批处理任务的管理，可以链接很多个任务，使它们自动化，并进行故障管理，这个做spark任务的人估计也需要，，后面几个看名字都知道和spark密不可分的。
除此之外，这个图还可以查询到某个用户所关注的项目以及这个项目中最多的语言。

> 找出热门语言

现在问题来了，怎么查询当前图中所有项目最热门的语言？我们可以查找每个项目然后抽取语言放入到counter计数器里面统计，这个看起来也不费事，但如果要查询有多少用户以某种语言编程呢？这就要扫描每个项目的入度用户，最后统计求和，时间复杂度挺大的。这个时候一种好的思路就是扩充图，将语言单独作为一个节点，最终图的逻辑图如下： 
![](https://i.imgur.com/tMVKGWU.png)
<pre>
repos = [n 
         for n in g.nodes_iter() 
             if g.node[n]['type'] == 'repo']

for repo in repos:
    # 消除None,有些空项目语言为None
    lang = (g.node[repo]['lang'] or "") + "(lang)"
    # 加星于repo的用户
    stargazers = [u 
                  for (u, r, d) in g.in_edges_iter(repo, data=True) 
                     if d['type'] == 'gazes'
                 ]
    for sg in stargazers:
        g.add_node(lang, type='lang')
        g.add_edge(sg, lang, type='programs')
        g.add_edge(lang, repo, type='implements')   
</pre>

接下来看下对语言的统计信息
<pre>
print (nx.info(g),'\n')
# 看下图里有什么语言
print ([n 
       for n in g.nodes_iter() 
           if g.node[n]['type'] == 'lang'])
# 某个用户使用的语言
print ([n 
       for n in g['ocanbascil(user)'] 
           if g['ocanbascil(user)'][n]['type'] == 'programs'],'\n')
# 查询最热门的语言
print ("Most popular languages")
print (sorted([(n, g.in_degree(n))
 for n in g.nodes_iter() 
     if g.node[n]['type'] == 'lang'], key=itemgetter(1), reverse=True)[:10])
# 查询用某种语言的有多少人
python_programmers = [u 
                      for (u, l) in g.in_edges_iter('Python(lang)') 
                          if g.node[u]['type'] == 'user']
print ("Number of Python programmers:", len(python_programmers))

javascript_programmers = [u for 
                          (u, l) in g.in_edges_iter('JavaScript(lang)') 
                              if g.node[u]['type'] == 'user']
print ("Number of JavaScript programmers:", len(javascript_programmers))

# 两个语言都用的人
print ("Number of programmers who use JavaScript and Python")
print (len(set(python_programmers).intersection(set(javascript_programmers))))

# 只用python不用js的人
print ("Number of programmers who use JavaScript but not Python")
print (len(set(javascript_programmers).difference(set(python_programmers))))

# XXX: Can you determine who is the most polyglot programmer?
</pre>


	Name: Type: DiGraph Number of nodes: 12807 Number of edges: 31824 Average in degree: 2.4849 Average out degree: 2.4849
	
	[‘LiveScript(lang)’, ‘APL(lang)’, ‘Swift(lang)’, ‘Java(lang)’, ‘Nix(lang)’, ‘Erlang(lang)’, ‘Common Lisp(lang)’, ‘Python(lang)’, ‘GCC Machine Description(lang)’, ‘Prolog(lang)’, ‘PigLatin(lang)’, ‘Vala(lang)’, ‘SAS(lang)’, ‘FORTRAN(lang)’, ‘Cuda(lang)’, ‘Matlab(lang)’, ‘HCL(lang)’, ‘F#(lang)’, ‘Verilog(lang)’, ‘Emacs Lisp(lang)’, ‘Inno Setup(lang)’, ‘Vue(lang)’, ‘TeX(lang)’, ‘DTrace(lang)’, ‘Processing(lang)’, ‘Hack(lang)’, ‘Lua(lang)’, ‘Assembly(lang)’, ‘Parrot(lang)’, ‘Shell(lang)’, ‘Arduino(lang)’, ‘R(lang)’, ‘C#(lang)’, ‘Rust(lang)’, ‘Standard ML(lang)’, ‘Puppet(lang)’, ‘Gosu(lang)’, ‘C(lang)’, ‘PHP(lang)’, ‘Scala(lang)’, ‘Gnuplot(lang)’, ‘Crystal(lang)’, ‘Objective-J(lang)’, ‘COBOL(lang)’, ‘Cucumber(lang)’, ‘ApacheConf(lang)’, ‘Brainfuck(lang)’, ‘Kotlin(lang)’, ‘Pascal(lang)’, ‘Pike(lang)’, ‘RAML(lang)’, ‘Lean(lang)’, ‘Logos(lang)’, ‘Elixir(lang)’, ‘Dart(lang)’, ‘C++(lang)’, ‘CMake(lang)’, ‘Clojure(lang)’, ‘D(lang)’, ‘Batchfile(lang)’, ‘OpenSCAD(lang)’, ‘Coq(lang)’, ‘PowerShell(lang)’, ‘Visual Basic(lang)’, ‘VimL(lang)’, ‘NetLogo(lang)’, ‘OCaml(lang)’, ‘VHDL(lang)’, ‘Ruby(lang)’, ‘Go(lang)’, ‘Metal(lang)’, ‘Jupyter Notebook(lang)’, ‘KiCad(lang)’, ‘Smarty(lang)’, ‘Tcl(lang)’, ‘CoffeeScript(lang)’, ‘QML(lang)’, ‘(lang)’, ‘Scheme(lang)’, ‘HTML(lang)’, ‘Makefile(lang)’, ‘CartoCSS(lang)’, ‘DIGITAL Command Language(lang)’, ‘AppleScript(lang)’, ‘Perl6(lang)’, ‘Protocol Buffer(lang)’, ‘Julia(lang)’, ‘Awk(lang)’, ‘Elm(lang)’, ‘Haskell(lang)’, ‘TLA(lang)’, ‘Nimrod(lang)’, ‘JavaScript(lang)’, ‘Max(lang)’, ‘TypeScript(lang)’, ‘GAP(lang)’, ‘Scilab(lang)’, ‘Web Ontology Language(lang)’, ‘XSLT(lang)’, ‘Objective-C++(lang)’, ‘Perl(lang)’, ‘PLSQL(lang)’, ‘PureScript(lang)’, ‘PLpgSQL(lang)’, ‘Eagle(lang)’, ‘Objective-C(lang)’, ‘Racket(lang)’, ‘OpenEdge ABL(lang)’, ‘Groovy(lang)’, ‘Groff(lang)’, ‘Frege(lang)’, ‘NSIS(lang)’, ‘Mathematica(lang)’, ‘CSS(lang)’]
	
	[‘JavaScript(lang)’, ‘R(lang)’, ‘Python(lang)’, ‘Jupyter Notebook(lang)’, ‘VimL(lang)’, ‘Scala(lang)’, ‘Clojure(lang)’, ‘CoffeeScript(lang)’, ‘Go(lang)’, ‘(lang)’, ‘CSS(lang)’]
	
	Most popular languages
	[(‘Python(lang)’, 80), (‘(lang)’, 74), (‘JavaScript(lang)’, 74), (‘Jupyter Notebook(lang)’, 69), (‘Java(lang)’, 67), (‘HTML(lang)’, 67), (‘C++(lang)’, 66), (‘Scala(lang)’, 65), (‘Shell(lang)’, 62), (‘C(lang)’, 62)]
	
	Number of Python programmers: 80
	Number of JavaScript programmers: 74
	Number of programmers who use JavaScript and Python
	74
	Number of programmers who use JavaScript but not Python
	0

从结果看出，github上喜欢的语言排行榜中js、r、python等脚本语言占据主流，虽然参与统计的项目只有一万多条，但这也应着了一句老话“github是前端的天下”😁。这里Scala的关注度也很靠前，主要因为“项目之源”是findspark，go这里也很火，java死哪去了。。。，估计挖个hadoop的相关项目就有他了。。。
另一个有趣的现象是，玩python的程序员必会玩js，玩js的人不一定会玩python（80-74=6个人不会玩python)，正符合现在IT界的现状啊，这个差别不明显，换个国内的项目估计就明显了。。。，估计还有多少人不知道github，这里就统计不了。

以上数据规模并不算大，如果有兴趣可以访问GitHub Archive,GitHub Archive 提供海量全局层面的数据，调可以使用一些推荐的“大数据”工具来探索它。



###	15.1.6 可视化

依靠Networkx的导出能力，可以通过JSON的JavaScript工具包D3使用，但也有许多其他的工具包，你可以考虑当可视化图。Graphviz是高度可配置的经典工具，可以设计出非常复杂的图形和位图图像。传统上在终端上运行，但现在也有支持大多数平台的用户界面。另一个流行的开源项目是Gephi，其在交互上做的很好，在过去的几年中Gephi已经迅速流行，非常值得推荐。 这里由于有上万个项目结点，一张图乌漆嘛黑都看不到，并且很卡。所以下面只截取用户点和语言点的可视化。

<pre>
import os
import json
from IPython.display import IFrame
from IPython.core.display import display 
from networkx.readwrite import json_graph
print ("Stats on the full graph" )
print (nx.info(g))
# 只提取用户和语言结点
mtsw_users = [n for n in g if g.node[n]['type'] == 'user'] +[n for n in g if g.node[n]['type'] == 'lang'] 
h = g.subgraph(mtsw_users)
print ("Stats on the extracted subgraph" )
print (nx.info(h))
# json导出
d = json_graph.node_link_data(h)
json.dump(d, open('force.json', 'w'))
viz_file = 'files/force.html'
# D3可视化
display(IFrame(viz_file, '100%', '900px'))
</pre>

下面的动态页面加载不出请点击动态图
![](https://i.imgur.com/VjCO3JV.png)

如果要分析语言之间的关联，其实也可以从一个项目中拿到多个语言，比如是a,b,c,然后做无向图，标记a-b-c 权重为1，当在其他项目中还有a,b同时出现时,a-b 权重为2，以此类推，最后直接放mjwillson的成品:
![](https://i.imgur.com/gkmbmIl.png)




## 15.2 微博话题爬取与存储分析
大数据社会下数据就是黄金，新浪微博作为一个国内网络社交早就意识到这一点，本着资本家和商人的心态给你提供的开放API接口只可以获得少量无关紧要的数据（想要数据，money来换），对比国外Twitte等社交平台会提供一些数据接口供研究人员获取大量研究数据。那我们GEEK的口号是，凡是网上能显示数据的朕兼“可取”(v_v…为什么加个引号呢，因为虽然出于技术角度是都可取得，但出于道德方面考虑也要尊重数据作者的规约）。

本文基于python以新浪微博为数据平台，从数据采集、关键字提取、数据存储三个角度，用最简单的策略来挖掘我们的“黄金”。

有爬虫基础的人可以直接跳过数据采集部分看“上海租房”话题挖掘实战项目，项目地址https://github.com/luzhijun/weiboSA（目前已更新豆瓣小组爬取）。
### 15.2.1 数据采集
使用python是因为代码简洁，虽然计算比java和c慢很多，但数据采集时间开销大部分是IO部分的，你愿意每次用java或者c写效率也提高不到哪去。

数据采集基本用爬虫机器人，原理谁都会，google就是靠他发家致富走上人生巅峰的。下面介绍常用来做爬虫的几个库。
> Urllib

怎样抓网页呢？其实就是根据URL来获取它的网页信息，虽然我们在浏览器中看到的是一幅幅优美的画面，但是其实是由浏览器解释才呈现出来的，实质它是一段HTML代码，加 JS、CSS，如果把网页比作一个人，那么HTML便是他的骨架，JS便是他的肌肉，CSS便是它的衣服。所以最重要的部分是存在于HTML中的，下面我们就写个例子来扒一个网页下来。

<pre>
import urllib2
response = urllib2.urlopen("http://www.baidu.com")
print response.read()
</pre>

结果就和在Chrome等浏览器中右键查看源码一样的内容，urllib2是python内置库，简化了httplib的用法(urllib2.urlopen相当于Java中的HttpURLConnection)。有2那肯定有urllib啊，urllib2可以接受一个Request类的实例来设置URL请求的headers，但urllib仅可以接受URL。这意味着，你不可以伪装你的User Agent字符串等。urllib2在python3.x中被改为urllib.request。 接下来用urllib2伪装iphone 6浏览，模拟浏览器发送GET请求。

<pre>
req = request.Request('http://www.douban.com/')
req.add_header('User-Agent', 'Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25')
with request.urlopen(req) as f:
    print('Status:', f.status, f.reason)
    print('Data:', f.read().decode('utf-8'))
</pre>

结果会返回移动版的源码信息

<pre>
...
<link rel="apple-touch-icon-precomposed" href="https://gss0.bdstatic.com/5bd1bjqh_Q23odCf/static/wiseindex/img/screen_icon.png"/>  
<meta name="format-detection" content="telephone=no"/>
...
</pre>
如果想要以post方式提交，只要在Request中附加data字段就可以，下面附加用户名密码登录新浪博客。
<pre>
#我们模拟一个微博登录，先读取登录的邮箱和口令，然后按照weibo.cn的登录页的格式以username=xxx&password=xxx的编码传入：
from urllib import parse
print('Login to weibo.cn...')
email = input('Email: ')
passwd = input('Password: ')
login_data = parse.urlencode([
    ('username', email),
    ('password', passwd),
    ('entry', 'weibo'),
    ('client_id', ''),
    ('savestate', '1'),
    ('ec', ''),
    ('pagerefer', 'https://passport.weibo.cn/signin/welcome?entry=mweibo&r=http%3A%2F%2Fm.weibo.cn%2F')
])

req = request.Request('https://passport.weibo.cn/sso/login')
req.add_header('Origin', 'https://passport.weibo.cn')
req.add_header('User-Agent', 'Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25')
req.add_header('Referer', 'https://passport.weibo.cn/signin/login?entry=mweibo&res=wel&wm=3349&r=http%3A%2F%2Fm.weibo.cn%2F')

with request.urlopen(req, data=login_data.encode('utf-8')) as f:
    print('Status:', f.status, f.reason)
    for k, v in f.getheaders():
        print('%s: %s' % (k, v))
    print('Data:', f.read().decode('utf-8'))
</pre>
其中Origin和referer字段是反“反盗链”，就是检查你发送请求的header里面，referer站点是不是他自己。

> Cookielib

爬虫被封的一个依据就是重复IP，因此可以为爬虫设置不同代理IP。此外有些网站需要cookie才能查看，所谓Cookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密）。比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容是不允许的。那么我们可以利用Urllib2库保存我们登录的Cookie，然后再抓取其他页面就达到目的了。

cookielib模块的主要作用是提供可存储cookie的对象，以便于与urllib2模块配合使用来访问Internet资源。Cookielib模块非常强大，我们可以利用本模块的CookieJar类的对象来捕获cookie并在后续连接请求时重新发送，比如可以实现模拟登录功能。该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。

它们的关系：CookieJar–派生->FileCookieJar –派生–>MozillaCookieJar和LWPCookieJar

<pre>
from urllib import request
from http.cookiejar import CookieJar

cookie=CookieJar()
cookie_support= request.HTTPCookieProcessor(cookie)#cookie处理器
opener = request.build_opener(cookie_support)
opener.open('http://www.baidu.com')
for item in cookie:
    print(item.name,':',item.value)
</pre>
结果：

	BAIDUID : E4DECD4AF63915B9AFF5AC28951A3DAA:FG=1
	BIDUPSID : E4DECD4AF63915B9AFF5AC28951A3DAA
	H_PS_PSSID : 1437_18241_17944_21079_18559_21454_21406_21377_21191_21321
	PSTM : 1477631558
	BDSVRTM : 0
	BD_HOME : 0
这里使用默认的CookieJar 对象，如果要将Cookie保存起来，可以使用FileCookieJar类和其子类中的save方法，加载就用load方法。

写脚本从指定网站抓取数据的时候，免不了会被网站屏蔽IP。所以呢，就需要有一些IP代理。随便在网上找了一个提供免费IP的网站西刺做IP抓取。观察可以发现有我们需要的信息的页面url有下面的规律：www.xicidaili.com/nn/+页码。可是你如果直接通过get方法访问的话你会发现会出现500错误。原因其实出在这个规律下的url虽然都是get方法获得数据，但都有cookie认证，另外还有反外链等，下面例子用来获得西刺的cookie。

<pre>
headers=[('User-Agent','Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25'),
    ('Host','www.xicidaili.com'),
    ('Referer','http://www.xicidaili.com/n')]
def getCookie()
    cookie=CookieJar()
    cookie_support= request.HTTPCookieProcessor(cookie)#cookie处理器
    opener = request.build_opener(cookie_support)
    opener.addheaders=headers
    opener.open('http://www.xicidaili.com/')
    return cookie
</pre>

有了cookie就可以爬了，爬的内容怎么处理呢，介绍个SB工具—— BeautifulSoup。

> BeautifulSoup

BeautifulSoup翻译叫鸡汤，现在版本是4.5.1，简称BS4，倒过来叫4SB，不过抓数据一点都不SB。提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为utf-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。 关于BS的介绍和用法官方文档很详细，下面给几个”Web scraping with python”1中的例子看下BS是否好喝，可以和文档对照看。 首先你得安装了BS，然后爬取http://www.pythonscraping.com/pages/page3.html中的图片来小试牛刀。

<pre>
import re
from urllib import request
from bs4 import BeautifulSoup

html=request.urlopen("http://www.pythonscraping.com/pages/page3.html")
bs=BeautifulSoup(html,"lxml")
#打印所有图片地址
for pic in bs.find_all('img',{'src':re.compile(".*\.jpg$")}):
    print(pic['src'])
</pre>
结果:
	
	../img/gifts/logo.jpg
	../img/gifts/img1.jpg
	../img/gifts/img2.jpg
	../img/gifts/img3.jpg
	../img/gifts/img4.jpg
	../img/gifts/img6.jpg
接上文，我们把西刺的高匿代理ip爬出来放到本地proxy.txt。

<pre>
cookie=getCookie()
# get the proxy
with open('proxy.txt', 'w') as f:
    for page in range(1,101):
        if page%50==0:#每50页更新下cookie
            cookie=getCookie()

        url = 'http://www.xicidaili.com/nn/%s' %page
        cookie_support= request.HTTPCookieProcessor(cookie)
        opener = request.build_opener(cookie_support)
        request.install_opener(opener)

        req = request.Request(url,headers=dict(headers))
        content = request.urlopen(req)
        soup = BeautifulSoup(content,"lxml")
        trs = soup.find('table',id="ip_list").findAll('tr')
        for tr in trs[1:]:
            tds = tr.findAll('td')
            ip = tds[1].text.strip()
            port = tds[2].text.strip()
            protocol = tds[5].text.strip().
            f.write('%s://%s:%s\n' % (protocol, ip, port))
</pre>

结果十五秒爬了1万条数据（与电脑环境有关），说明1页正好100条，而总页数超过1000页，也就是记录数超过10w条，如果固定用同一个cookie肯定不安全（谁会有空翻看1000页数据。。。），因此设置每爬50页更新下cookie。 有了代理地址，不一定能保证有效，可能就被封杀了，因此使用思路是把代理地址存入哈希表，验证无效的删除（看状态码），重新在表中取新的记录。 代理地址使用方式如下：
<pre>
...
proxy_handler = request.ProxyHandler({'http': '123.165.121.126:81'}) #http://www.xicidaili.com/nn/2 随便找个
opener = request.build_opener(proxy_handler,cookie_handler ...各种其他handle)
...
</pre>
另外推荐个神器，crawlera ，基本满足各种需要。

假如真要爬1000页，需要花150秒？好吧，好像也不多，但我要说的是可以多进程或者异步处理。多进程很好做，注意以手动维护一个HttpConnection的池，然后每次抓取时从连接池里面选连接进行连接即可（每秒几百个连接正常的有理智的服务器一定会封禁你的）。python的异步处理用到了Twisted库，却远没有同是异步模式的nodejs火，算是python中的巨型框架了，想想python的巨型框架活的不久，感兴趣的推荐看下《Twisted网络编程必备》2。关于单线程、多线程、异步有张图推荐看下。
![](https://i.imgur.com/HWO629J.png)

写爬虫还要考虑其他很多问题，授权验证、连接池、数据处理、js处理等，这里有个经典爬虫框架：Scrapy，目前支持python3，支持分布式， 使用 Twisted来处理网络通讯，架构清晰，并且包含了各种中间件接口，可以灵活的完成各种需求。

> Scrapy与Pyspider

Scrapy的入门学习参见学习Scrapy入门，对应中文文档几小时内可以快速掌握。另外国内某大神开发了个WebUI的Pyspider，具有以下特性：

1. python 脚本控制，可以用任何你喜欢的html解析包（内置 pyquery）
1. WEB 界面编写调试脚本，起停脚本，监控执行状态，查看活动历史，获取结果产出
1. 支持 MySQL, MongoDB, SQLite
1. 支持抓取 JavaScript 的页面
1. 组件可替换，支持单机/分布式部署，支持 Docker 部署
1. 强大的调度控制


从内容上讲，两者具有功能差不多，包括以上3，5，6。不同是Scrapy原生不支持js渲染，需要单独下载scrapy-splash,而PyScrapy内置支持scrapyjs；PySpider内置 pyquery选择器，Scrapy有XPath和CSS选择器，这两个大家可能更熟一点；此外，Scrapy全部命令行操作，Pyscrapy有较好的WebUI；还有，scrapy对千万级URL去重支持很好，采用布隆过滤来做，而Spider用的是数据库来去重？最后，PySpider更加容易调试，scrapy默认的debug模式信息量太大，warn模式信息量太少，由于异步框架出错后是不会停掉其他任务的，也就是出错了还会接着跑。。。从整体上来说，pyspider比scrapy简单，并且pyspider可以在线提供爬虫服务，也就是所说的SaaS，想要做个简单的爬虫推荐使用它，但自定义程度相对scrapy低，社区人数和文档都没有scrapy强，但scrapy要学习的相关知识也较多，故而完成一个爬虫的时间较长。

因为比较喜欢有完整文档的支持，所以后面主要用Scrapy，简要说下Scrapy运行流程。

- 首先，引擎从调度器中取出一个链接(URL)用于接下来的抓取
- 引擎把URL封装成一个请求(Request)传给下载器，下载器把资源下载下来，并封装成应答包(Response)
- 然后，爬虫解析Response
- 若是解析出实体（Item）,则交给实体管道进行进一步的处理。
- 若是解析出的是链接（URL）,则把URL交给Scheduler等待抓取

根据scrapy文档描述,要防止scrapy被禁用，主要有以下几个策略。

1. 动态设置user agent
1. 禁用cookies
1. 设置延迟下载
1. 使用 Google cache
1. 使用IP地址池（ Tor project 、VPN和代理IP）
1. 使用 Crawlera

由于Google cache基于你懂的原因不可用，其余都可以利用，Crawlera的分布式下载，我们可以在下次用一篇专门的文章进行讲解。下面主要从动态随机设置user agent、禁用cookies、设置延迟下载和使用代理IP这几个方式入手。

> 自定义中间件


Scrapy下载器通过中间件控制的，要实现代理IP、user agent切换可以自定义个中间件。 在项目下创建（如何创建项目，使用scrapy start yourProject命令，参考文档）好项目后，在里面找到setting.py文件，先把agents和代理ip放到setting.py中（代理ip较少情况下这样做，较多的话还是放到数据库中去，方便管理）,设置中间件名字MyCustomSpiderMiddleware和优先级。

<pre>
USER_AGENTS = [
	"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
	"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)",
	"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
	"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)",
	"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)",
	"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)",
	"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)",
	"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)",
	"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6",
	"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1",
	"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0",
	"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5",
	"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6",
	"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11",
	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20",
	"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52",
]
PROXIES = [
	{'ip_port': '111.11.228.75:80', 'user_pass': ''},
	{'ip_port': '120.198.243.22:80', 'user_pass': ''},
	{'ip_port': '111.8.60.9:8123', 'user_pass': ''},
	{'ip_port': '101.71.27.120:80', 'user_pass': ''},
	{'ip_port': '122.96.59.104:80', 'user_pass': ''},
	{'ip_port': '122.224.249.122:8088', 'user_pass': ''},
]
# 禁用cookoe (enabled by default)
COOKIES_ENABLED = False

#设置下载延迟
DOWNLOAD_DELAY = 1

# 下载中间件
# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html
DOWNLOADER_MIDDLEWARES = {
    'weiboZ.middlewares.MyCustomDownloaderMiddleware': 543,
}
</pre>

middlewares/MyCustomDownloaderMiddleware.py
<pre>

import random
import base64
from settings import PROXIES
class RandomUserAgent(object):
	"""Randomly rotate user agents based on a list of predefined ones"""
	def __init__(self, agents):
		self.agents = agents
	@classmethod
	def from_crawler(cls, crawler):
		return cls(crawler.settings.getlist('USER_AGENTS'))
	def process_request(self, request, spider):
		#随机选个agent
		request.headers.setdefault('User-Agent', random.choice(self.agents))
class ProxyMiddleware(object):
	def process_request(self, request, spider):
		proxy = random.choice(PROXIES)
		if proxy['user_pass'] is not None:
			request.meta['proxy'] = "http://%s" % proxy['ip_port']
			encoded_user_pass = base64.encodestring(proxy['user_pass'])
			request.headers['Proxy-Authorization'] = 'Basic ' + encoded_user_pass
		else:
			request.meta['proxy'] = "http://%s" % proxy['ip_port']
</pre>

###	15.2.2 互联网道德和规约
当你准备爬某个网站的时候，首先应该先看下该网站有没有robots.txt。robots.txt是1994年出现的，也称为机器人排除标准(Robots Exclusion Standard)，网站管理员不想某些内容被爬到的时候可以再该文件中注明。robots.txt虽然有主流的语法格式，但是各大企业标准不一，没有别人可以阻止你创建自己版本的robots.txt，但这些robots.txt不应该因为不符合主流而不被遵守。一般文件字段包含：User-agent，Allow，Disallow分别代表搜索机器人允许看和不许看的内容。

之前看新闻说今年4月大众点评把百度给告了，请求法院判令两被告停止不正当竞争行为，共同赔偿汉涛公司经济损失9000万元和为制止侵权行为支出的45万余元，并刊登公告、澄清事实消除不良影响。有用百度地图的应该知道这个（最近百度高德开撕，又在黑百度了~~~），定位完毕会显示附近商家和点评信息，来看下大众点评网的robots.txt. 光看

	User-agent: *
	…
	Disallow: /shop//rank_p
	…

就知道不允许任何企业和个人爬他家的商店评分数据，更何况其他更具有价值的数据呢，数据是黄金，要求赔偿9000万元对百度来说不算多，但百度回应内容大众点评网的robots协议面向百度等搜索引擎开放，百度地图抓取大众点评网的内容正是在robots.txt允许的情况下。通常业内习惯上没有被不允许的就是允许的，也就是说网站的关键信息可以帮助SEO优化的这个不能被禁止哟，不然你就没头条了，看人家竞争对手爱帮网倒是单独被列出来全面封杀了，因为其实力太弱，没有商业合作价值。就算这样我也没看出允许百度抓点评的用户评论数据，难道说点评网之前没robots.txt？人家不傻！百度挖了人家数据还叫嚣着遵守Robots协议，（其实他完全可以偷偷摸摸抓了数据自己私下研究，却要直接在百度地图上显示出来，这是要把数据价值榨干啊，够霸道）好比把人打了顿理直气壮地说你瞅啥一样，太野蛮了。。。

说多了，来看下新浪微博的Robots协议。明确规定了Sitemap: http://weibo.com/sitemap.xml 中列出的内容不允许被百度、360、谷歌、搜狗、微软必应、好搜、神马查看，后面还注明了Disallow: User-agent: * Disallow: /，也就是说前面是单独列出的，理论上这些数据不允许任何机构和个人爬取。这些是啥数据呢，movie和music数据，那你放心好了，微博文本数据可以爬了，但人家也不傻，可以显示的微博信息是有限制的，不可能所有数据库的数据都显示出来。
> 实战

在58、赶集、链家上找过房子的人都为中介苦恼，所谓的行业规矩令人做呕，这些人不生产社会价值却担当了新世纪的买办角色，好在通过微博也可以找房，而且绝大部分是个人房源。

以上海找房子为例，微博搜索框输入@上海租房 就可以的到如下页面

	http://s.weibo.com/weibo/%2540%25E4%25B8%258A%25E6%25B5%25B7%25E7%25A7%259F%25E6%2588%25BF?topnav=1&wvr=6&b=1
还是不错的，然后看下源码发现并没有html数据，显然是AJAX异步了，Scrapy要爬的话还得安装scrapy-splash改下配置用splash解析js内容，而且要看下一页必须登录状态才可以，那要在header里面添加cookie，可以登录后chrome F12 开发工具查看，但你敢保证拿包含自己的账号的cookie去做爬虫发现了不被封？其实这里可以显示的数据最多1000条，按最新的1000条显示，何必大费周章去搞那么复杂呢，可以用移动版的微博搜下嘛,点击。

用开发者工具看下网络请求数据状况，搜索包含名字‘page’ 请求消息头，可以发现规律：
![](https://i.imgur.com/Rn6Hjya.png)
左边Name列凡是内容页下拉引起ajax加载新页，新页内容以json格式返回；右边字段末尾page=？部分，代表传递第几页的内容，?最大到100，和电脑版最多看50页一样有数据限制。
json内容如下：
![](https://i.imgur.com/GBWluq0.png)
ok，能显示的数据都在里面，而且还是json格式，都不用选择器了，这个要比电脑版简单多了。
###	15.2.3 数据提取
> 选择需要的数据

并不是所有json字段的数据都有用，这里只选取有用的字段，总的原则是按需抽取。可以看下项目中定义的Items.py。

微博内容id 对应字段放数据库中将有唯一约束，防止重复微博。选择mblogid作为唯一id，而千万不是itemid，经测试发现itemid只代表当天微博的槽位，比如限制浏览10条数据，就有1~10个槽位，而itemid就代表这10个槽位标签，并不代表微博内容id。另外mblog字段下还有个id属性，估计和mblogid一样的效果，有兴趣可以试试。
发布时间代表信息的实效，json里面有两个字段表示，一个是时间戳created_timestamp，另一个是显示出来的真实时间数据，这里取真实数据方便直接提取显示，但后期存储的时候需要统一转换为标准时间格式。 
评论数、转发数、点赞数和时效结合可以用来综合评估微博信息价值(时间越靠后这三个数字越能评价信息价值)。
用户名、粉丝数、说说数可以用来检验用户是否有价值用户，或者是机器人。
后期处理需要提取求/租信息的关键词，包含价格、几号线、行政区划、信息是求租还是出租。

项目中定义的pipelines.py文件是scrapy管道处理类，也就是主要的后期数据处理类。其中一个是JsonPipeline类，直接将数据打印到json文件中，这个前期可以用来调试爬虫效果。另一个是MongoPipeline类，用来保存后期处理后的数据。在setting文件中ITEM_PIPELINES属性可以设置具体采用哪个管道处理类。

后期处理主要任务是提取关键字，如何从微博信息中爬取地理位置、价格？这里采用双数组Trie树

> DAT

Trie树是搜索树的一种，来自英文单词”Retrieval”的简写，可以建立有效的数据检索组织结构，是中文匹配分词算法中词典的一种常见实现。它本质上是一个确定的有限状态自动机（DFA），每个节点代表自动机的一个状态。在词典中这此状态包括“词前缀”，“已成词”等。前面文章讲了下其原理，可以查看。

采用Trie树搜索最多经过n次匹配即可完成一次查找(即最坏是0(n))，而与词库中词条的数目无关，缺点是空间空闲率高，它是中文匹配分词算法中词典的一种常见实现。

双数组Trie（doublearrayTrie,DAT）是trie树的一个简单而有效的实现（日本人发明的），由两个整数数组构成，一个是base[]，另一个是check[]。双数组Trie树是Trie树的一种变形，是在保证Trie树检索速度的前提下，提高空间利用率而提出的一种数据结构.其本质是一个确定有限状态自动机(DeterministicFiniteAutomaton，DFA)，每个节点代表自动机的一个状态，根据变量的不同，进行状态转移，当到达结束状态或者无法转移时完成查询.DAT采用两个线性数组(base和check)对Trie树保存，base和check数组拥有一致的下标，即DFA中的每一个状态，也即Trie树中所说的节点，base数组用于确定状态的转移，check数组用于检验转移的正确性，检验该状态是否存在34。

在比较用于正向最大匹配分词的速度方面，DAT分词平均速度为936kB/s5（2006年），项目用到github上一日本人的python版的DAT，其查询速度可以达到 2.755M/s，查询速度和分词速度基本是差不多的，这三倍的差距应该是做了优化的。

词典的收集是比较麻烦，没有现成的，项目中搜集了上海地铁、街道、行政区、乡镇等信息，其中价格信息范围是从600~9000，可识别二千、二千二、两千一等中文价格，后面微博上看到有人用1.2k做价格的，暂时没加入，自己可以加入词条后重新运行下makeData.py文件即可收录。

判断信息是租房还是求房也是根据关键字，当信息中出现[“求租”, “想租”,”求到”,”求从”, “要租”, “寻租”,”寻找”, “找新房子”, “找房子”, “找房”, “寻房”, “求房”, “想找”, “希望房”]信息就标注为求房，否则标注为租房。

此外项目还收集了三千多个楼盘信息，由于有些楼盘信息容易混淆真实语境，比如‘峰会’（真不懂怎么会有这楼盘名）、‘艺品’与信息‘文艺品味’、‘黄兴’、‘金铭’与人名冲突等等。有想根据楼盘查询信息的同学可以把makeData.py中第5、51行注释取消运行下这个文件。

关于时间处理，微博挖到的时间有几种类型：

- 2016年01月01日 00点00分
- 1月1日 00：00
- 今天 00：00
- 1分钟前/11分钟前
- 10秒前

需要统一转化，使用DataUtil类处理。其中mongodb使用的是ISO时间，比北京时间早8小时，而pymongo中的datetime.datetime 数据并不会按时区处理，因此手动减少8小时后存储。同样从mongoDB中取出的时间要转化为当地时间。

<pre>
> d=new Date()
> d
ISODate("2016-10-29T06:59:49.461Z")
> d.toLocaleDateString()
10/29/2016
</pre>

###	15.2.4 数据存储
其实就这点数据放哪个数据库都无所谓，但假如这个数据量很大，就要好好考虑数据存储了。
> 选择oracle、mysql 还是 nosql

数据库的比较就好比java、c#、python、Go等的骂战一样，没有最好的，只有最适合场景的。oracle、mysql都学过，nosql中学过hbase和mongodb，就我而言单从7个角度比较：

1. 功能：oracle>mysql> nosql
1. 写性能：noSql>oracle≈≈mysql
1. 简单查询: oracle>mysql>nosql
1. 复杂查询(含join): oracle>mysql>nosql
1. 架构扩展: noSql>mysql>oracle
1. 可维护性: oracle>mysql>nosql
1. 成本: oracle>mysql≈≈nosql

对于现在这个场景，爬虫在前端爬数据，管道层在那边处理数据后写数据，而这些数据具有时效性，也就是说只会去读一部分数据，相对来说，这就对写的要求较高。此外，这个场景就一个表，不涉及多表关联、约束等，复杂查询可以说没有，需要功能较少。另外网络数据不能保证一致性和可靠性，只要高可用性(HA)即可，Nosql可以设置副本机制达到高可用性，mysql虽然也可以做到成本稍高，将来可扩展角度也不适合。因此这个场景最适合的是Nosql。

> Hbase 还是Mongodb

Cassandra HBase和MongoDb性能比较此文详细比较了三种主流Nosql数据库，最终项目选择Mongodb，就在于MongoDB适合做读写分离场景中的读取场景，并且其用js开发的，对json插入支持特别好。什么时候mongodb是较坏的选择呢，参考WHY MONGODB IS A BAD CHOICE FOR STORING OUR SCRAPED DATA

python的mongodbSDK包叫pymongo，十分钟看个教程就会了，这个业务场景为了加快查询，需要对价格、行政区、发布时间创建索引，其中价格、行政区由于是数组形式所以是多键索引，索引属性是稀疏的，即不允许空值。此外对这条微博的mblog_id加个唯一索引。索引在初始运行时创建，之后除非手动删除数据库后运行，否则不会再创建。

为保证每次插入的数据都是最新的，插入前应比较数据的发布时间与数据库中的最新时间，如果是早的说明已经爬过的，不需要插入。

关于mongodb的使用文档，点这里。

###	15.2.5 项目运行与分析
> 运行项目

将项目git到本地后，请先确保以下环境已经安装：

- scrapy: [https://scrapy.org/](https://scrapy.org/)
- datrie: [https://github.com/pytries/datrie](https://github.com/pytries/datrie)
- pymongo: [https://github.com/mongodb/mongo-python-driver](https://github.com/mongodb/mongo-python-driver)
- mongoDB: [https://www.mongodb.com/](https://www.mongodb.com/)

执行下面命令：

	mongod
	cd weiboSA
	scrapy crawl mblogSpider

可选参数：

	scrapy crawl mblogSpider -a num= -a new_url=

- num 代表爬取页面数，默认为100页，目前只支持100页。

- newurl 默认为搜索移动端‘上海租房’返回的json文件url，如果要添加其他上海租房信息，比如浦东租房，请自行在Chrome中找到请求的json地址，例如：
	http://m.weibo.cn/page/pageJson? 
	containerid=&containerid=100103type%3D1%26q%3D浦东租房
	&type=all 
	&queryVal=浦东租房 
	&luicode=10000011 
	&lfid=100103type%3D%26q%3D上海无中介租房 
	&title=浦东租房 
	&v_p=11 
	&ext=
	&fid=100103type%3D1%26q%3D浦东租房
	&uicode=10000011
	&next_cursor=
	&page=
	如果要数据库收录‘浦东租房’历史记录信息，请将pipelines.py第87、88行注释掉。一般如果有‘上海租房’了就不要去搜索‘浦东租房’，因为基本上有‘浦东租房’的微博都会有@‘上海租房’，所以下面会出现插入重复记录错误。

<pre>
➜  weiboZ git:(master) ✗ scrapy crawl mblogSpider -a num=10 -a new_url="http://m.weibo.cn/page/pageJson\?containerid\=\&containerid\=100103type%3D1%26q%3D%E6%B5%A6%E4%B8%9C%E7%A7%9F%E6%88%BF\&type\=all\&queryVal\=%E6%B5%A6%E4%B8%9C%E7%A7%9F%E6%88%BF\&luicode\=10000011\&lfid\=100103type%3D%26q%3D%E4%B8%8A%E6%B5%B7%E6%97%A0%E4%B8%AD%E4%BB%8B%E7%A7%9F%E6%88%BF\&title\=%E6%B5%A6%E4%B8%9C%E7%A7%9F%E6%88%BF\&v_p\=11\&ext\=\&fid\=100103type%3D1%26q%3D%E6%B5%A6%E4%B8%9C%E7%A7%9F%E6%88%BF\&uicode\=10000011\&next_cursor\=\&page\="
2016-10-29 14:41:11 [root] WARNING: 生成MongoPipeline对象
2016-10-29 14:41:11 [root] WARNING: 开始spider
2016-10-29 14:41:11 [root] WARNING: 允许插入数据的时间大于2016-10-29 14:15:05.875000
2016-10-29 14:41:13 [root] WARNING: do page1.
2016-10-29 14:41:13 [root] WARNING: do other pages.
2016-10-29 14:41:13 [root] ERROR: 编号为:E91f233Ds的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:Ef4ri5bC6的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:Ef3UNqMmV的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:Ef3stkA8a的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:Ef3pzmJ6i的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:Ef1OBtvQr的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:Ef03Lj54z的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:EeYLU2GQd的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:EeYlBv7bn的数据插入异常
2016-10-29 14:41:13 [root] ERROR: 编号为:EeXkop2vu的数据插入异常
2016-10-29 14:41:15 [root] WARNING: 结束spider
</pre>
更改日志显示级别请在setting.py中修改LOG_LEVEL，介意采用项目默认的WARNNING，否则信息会很多。

> 查询示例

查询当前时区的2016-10-20至今有在9号线附近租房房租不高于2000的信息。

<pre>
db.house.find(
{
	created_at:{$gt:new Date('2016-10-20T00:00:00')},
	$or:
		[
			{price:{$lte:2000}},
			{price:[]}
		],
	admin:'9号线',
	tag:true
},
{	
	_id:0,
	text:1,
	created_at:1,
	scheme:1
}
).hint('created_at_-1').pretty()

{
	"text" : "房子在大上海国际花园，漕宝路1555弄，距9号线合川路地铁站步行5分钟，距徐家汇站只有4站，现在转租大床，有独立卫生间，公共厨房，房租2400，平摊下来1200，有一女室友，室友宜家上班，限女生，没有物业费，包网络，水电自理@上海租房无中介 @上海租房无中介 @上海租房 @上海租房无中介联盟",
	"scheme" : "http://m.weibo.cn/1641537045/EetVm3WBV?",
	"created_at" : ISODate("2016-10-25T09:18:00Z")
}
{
	"text" : "#上海租房##上海出租#9号线松江泗泾地铁站金地自在城，12层，步行、公交或小区班车直达地铁站。精装，品牌家具家电，主卧1800RMB/月；公寓门禁出入，房东直租，电话：13816835869，或QQ：36804408。@上海租房 @互助租房 @房天下上海租房 @上海租房无中介   @应届毕业生上海租房",
	"scheme" : "http://m.weibo.cn/1641537045/Een8cAoy8?",
	"created_at" : ISODate("2016-10-24T16:00:00Z")
}
{
	"text" : "#上海租房# 个人离开上海：转租地铁9号线朝南主卧带大阳台，离地铁站两分钟！设备齐全，交通方便，随时入住。具体信息看图片～@上海租房 @上海租房无中介联盟 @魔都租房 帮转谢谢！",
	"scheme" : "http://m.weibo.cn/1641537045/EdRpfuKuH?",
	"created_at" : ISODate("2016-10-21T07:14:00Z")
}
{
	"text" : "9号线桂林路 离地铁站8分钟 招女生室友哦 @上海租房 @上海租房无中介联盟 上海·南京西路",
	"scheme" : "http://m.weibo.cn/1641537045/EdJ2U8Kv3?",
	"created_at" : ISODate("2016-10-20T09:57:00Z")
}
</pre>

> Note


python 的第三方requests库使用起来比自带的urllib更容易，是对urlib的进一步封装，读者可以自己尝试，这里不再举例。
在spider文件夹目录下可自建爬虫，爬取像豆瓣租房小组类似信息加入数据库。
数据分析部分比如如何识别微博机器人，如何构建信息评价指标等，每个人实现方案不一样，挖掘信息的程度不同而已，本文不予给出。
可设置定时任务，比如一般上海租房每天更新两页，就定时运行命令并且num=2。
技术分享，全篇五千多字欢迎转载，但请注明出处，否则，否则我哭给你看😢。。。






## 15.3 Google Plus 文本提取与分析
本文所有数据源自google+,全篇围绕五个方面来进行文本提取和分析：

1. 数据获取
1. 中文分词
1. NLTK
1. 特征词提取
1. 文本相似度

除此之外本文还设计到情感词分析，齐普夫定律等。其他方法像摘要自动提取、意见挖掘、文本聚类、新闻分类等常规文本分析内容并不适合google+的数据集，因此本文没有涉及。

### 15.3.1 获取数据

> 准备

google+ api 获取授权：
![](https://i.imgur.com/HUdxxZ7.png)

左侧栏第三个，点“凭据”，创建api密钥。这里用api密钥就可以，毕竟只采集公共数据信息，不是用户隐私数据（需要用户认证）。
![](https://i.imgur.com/4H2Jf3H.png)

Google+ API文档中具体列出了使用方法。此外，有专门的API浏览器可以根据查询条件快速生成RESTful对应的url地址，得到查询结果。(使用之前先设置右上角的API id)

> RESTful API

以查询某个用户为例。

- 使用API浏览器
![](https://i.imgur.com/QS7eT0G.png)

![](https://i.imgur.com/WitGNSD.png)

- 直接浏览器抓取

<pre>
curl "https://www.googleapis.com/plus/v1/people?query=Trucy+Luce&key=$api_key"
</pre>

	{
	“kind”: “plus#peopleFeed”,
	“etag”: “"xw0en60W6-NurXn4VBU-CMjSPEw/ycgzanUve1bCMxC0NXlP6C4MSTM"”,
	“selfLink”: “https://www.googleapis.com/plus/v1/people?query=Trucy+Luce&key=AIzaSyB2_mjqVwT5IpqGO8wfASMdnHUuvxwQqlI”,
	“title”: “Google+ People Search Results”,
	“nextPageToken”: “CAESFTExMjAzMzk2OTk4NDI0NTc3MzQ3NQ”,
	“items”: [
	{
	“kind”: “plus#person”,
	“etag”: “"xw0en60W6-NurXn4VBU-CMjSPEw/xLkYpT8h4zlkDrhCUQcTzEZDl1U"”,
	“objectType”: “person”,
	“id”: “112033969984245773475”,
	“displayName”: “Trucy Luce”,
	“url”: “https://plus.google.com/112033969984245773475”,
	“image”: {
	“url”: “https://lh3.googleusercontent.com/-L0oIZ63NyHk/AAAAAAAAAAI/AAAAAAAAABg/xr0lpRdSDLM/photo.jpg?sz=50”
	}
	}
	]
	}

关于json中key的解释，可以在API文档中找到。

> 利用python SDK

google api for python 有两个版本，一个是oauth2授权的，另一个就是简单的api_key授权的。我们使用第二个，这是其API文档。该客户端已经支持python3版本，这里本文代码都以python3为基础。

<pre>
import httplib2
import json
import apiclient.discovery # pip install google-api-python-client

Q = "Trucy Luce"
API_KEY = '自己输入' 
service = apiclient.discovery.build('plus', 'v1', http=httplib2.Http(), developerKey=API_KEY)
people_feed = service.people().search(query=Q).execute()
print (json.dumps(people_feed['items'], indent=2))
</pre>

build方法详细查看代码中的api说明，返回一个与服务交互的资源对象（” A Resource object with methods for interacting with the service. “ ）。而这个对象实际上是调动了 build_from_documen这个方法，这个方法返回的是googleapiclient.discovery.Resource资源对象，定位到这个Resource资源对象，但仔细查找发现其方法下面没有people()方法，但有三个生成方法的方法：

	_add_basic_methods
	_add_nested_resources
	_add_next_methods

实际上这三个方法是根据response的schem层次结构来生成的，这里的代码值得学习。 最后只打印items部分，程序执行结果如下:

	[
	{
	“etag”: “"xw0en60W6-NurXn4VBU-CMjSPEw/xLkYpT8h4zlkDrhCUQcTzEZDl1U"”,
	“displayName”: “Trucy Luce”,
	“objectType”: “person”,
	“image”: {
	“url”: “https://lh3.googleusercontent.com/-L0oIZ63NyHk/AAAAAAAAAAI/AAAAAAAAABg/ xr0lpRdSDLM/photo.jpg?sz=50”
	},
	“id”: “112033969984245773475”,
	“kind”: “plus#person”,
	“url”: “https://plus.google.com/112033969984245773475”
	}
	]
接下来挖掘下某个用户的近期活动，实际上就像推特或者一条说说。这次挑个说中文的“Mulin Hong”。

<pre>
USER_ID = '102121409478764742904' # Mulin Hong
service = apiclient.discovery.build('plus', 'v1', http=httplib2.Http(), developerKey=API_KEY)
activity_feed = service.activities().list(
  userId=USER_ID,
  collection='public',
  maxResults='100' # 最大允许记录条数
).execute()
print (json.dumps(activity_feed, ensure_ascii=False,indent=2))
</pre>
service.activities().list方法有几个必要参数，userId:活动发起者Id。可选参数：collection:活动归属的集,默认为public，还是看下面图片吧：

![](https://i.imgur.com/gcHi9iM.png)

其他参数可以通过命令help(service.activities().list)来查看。 由于共7000多行，不方便直接放入文章，可以点击查看内容。

如果仔细观察内容，你会发现content中有许多html标签，像<br>之类的在文本中应该去除，这里用到python Beautifulsoup4包1,(Beautiful Soup)已成为和lxml、html6lib一样出色的python解释器，为用户灵活地提供不同的解析策略或强劲的速度，提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。简单来说，Beautiful Soup是python的一个库，最主要的功能是从网页抓取数据。更多关于Beautiful Soup的内容可以参见学习教程2。

随便拿条内容:
![](https://i.imgur.com/nvPBGEb.png)

可以发现里面除了含有html标签还有BOM字符(\ufeff)，使用bs4去除：
![](https://i.imgur.com/RSV83Wg.png)

我们上面只拿了100条记录，能不能拿全部记录呢？我们在看一下爬下的json文件，发现第一行显示nextPageToken，也就是说还有下一页，我们可以通过service.activities().list_next方法拿到下一页的resopnse。最后我们拿到所有json并用上面方法清洗整理。

<pre>
import os
import httplib2
import json
import apiclient.discovery
from bs4 import BeautifulSoup

USER_ID = '102121409478764742904' # Mulin Hong
MAX_RESULTS = 400 # 最大纪录数，每页100条，也就是说最多5页（0~4页）
API_KEY = '你自己的api_key' 

def cleanHtml(html):
  if html == "": return ""
  return BeautifulSoup(html,'lxml').get_text()[:-1]

service = apiclient.discovery.build('plus', 'v1', http=httplib2.Http(), 
                                    developerKey=API_KEY)
activity_feed = service.activities().list(userId=USER_ID,
  collection='public',
  maxResults='100' 
)

activity_results = []

while activity_feed != None and len(activity_results) < MAX_RESULTS:
  activities = activity_feed.execute()
  if 'items' in activities:
    for activity in activities['items']:
      if activity['object']['objectType'] == 'note' and activity['object']['content'] != '':        
        activity['title'] = cleanHtml(activity['title'])
        activity['object']['content'] = cleanHtml(activity['object']['content'])
        activity_results += [activity]
#查看下一页
  activity_feed = service.activities().list_next(activity_feed, activities)
# 写到一个json文件中去
f = open(os.path.join('resources', USER_ID + '.json'), 'w')
f.write(json.dumps(activity_results,ensure_ascii=False, indent=2))
f.close()
print (str(len(activity_results)), "activities written to", f.name)
</pre>
最后输出结果”422 activities written to resources/102121409478764742904.json”，共422条被输出。查看
### 15.3.2 中文分词

为了进一步对每条记录分析，有必要进行中文分词。文章3中提到11款开放中文分词引擎，从分词效果和调用难度角度考虑，这里采用商业化的BosonNLP工具（关键被他一句广告吸引“现在加入BosonNLP，可获得分词与词性标注引擎不限量调用额度！”）。 注册波森后会给你一个api_key，和google+一个原理。安装python版SDK： pip install -U bosonnlp 。api文档很简单，看一遍就直接用了。

引用方法：

<pre>
In [1]: from bosonnlp import BosonNLP

In [2]: nlp = BosonNLP('你自己的api_key')

In [3]: s='当年北京市长夸下海口： 2017年治不了雾霾就提头来见      2016年10月16日，北京市空气污染指数读数    嗯，
   ...: 北京市长同志，你只有两个多月的时间了……'

In [4]: nlp.tag(s)[0]['word']
Out[6]:
['当年',
 '北京',
 '市长',
 '夸',
 '下',
 '海口',
 '：',
 '2017年',
 '治',
 '不',
 '了',
 '雾霾',
 ...
</pre>
直接使用里面的api进行情感分析：

<pre>
from bosonnlp import BosonNLP
nlp = BosonNLP('你的api_key')
all_content = [a['object']['content'] for a in activity_results ]
emos=nlp.sentiment(all_content[:100], model='weibo')#单词最多只能分析100个，每日最多500个
drawdata=list(zip(*emos))[0]
</pre>

绘制箱线图和统计直方图：
![](https://i.imgur.com/J5P2HTo.png)
![](https://i.imgur.com/fOPo8eW.png)

从图中可以看出，整体情感偏上，这里只计算了100条，有兴趣把一天的免费记录数500条全计算完了看看结果。当然情感算法用的很普遍了，自己实现个也非难事。 hrKOofx5.10057.yUUmemEd43Dm

### 15.3.3 NLTK

斯坦福大学自然语言处理组是世界知名的NLP研究小组，他们提供了一系列开源的Java文本分析工具，包括分词器(Word Segmenter)，词性标注工具（Part-Of-Speech Tagger），命名实体识别工具（Named Entity Recognizer），句法分析器（Parser）等，可喜的事，他们还为这些工具训练了相应的中文模型，支持中文文本处理。NLTK4是针对python的一个自然语言处理平台,在windows、mac、linux平台上通用，并且是开源免费的，国内好多项目都是在它基础上做的。 下面用nltk分析下抓到的数据。

<pre>

import nltk
from bosonnlp import BosonNLP

nlp = BosonNLP('你自己的api_key')

#将所有记录转化为一个词语数组tokens
contents='。'.join(all_content)
tokens=nlp.tag(contents)[0]['word']
text = nltk.Text(tokens)

text.concordance("三星")#查看出现‘三星’的语句,可选参数包括语句数目和语句长度
'''
Displaying 4 of 4 matches:
 北京 市长 准备 如何 收场 ？ 。 # 看 图 不 说话 1016 ： 三星 手机 名不虚传 。 How to mess up # StarWars a
假 论文 ， 那 就 是 丢脸 了 。 # 看 图 不 说话 0923 ： 三星 一生 黑 http://cinacn.blogspot.com/2016/
 ： 心里 想 说 没事 了 。 # 看 图 不 说话 0917 ： 使用 三星 手机 的 后果 。 这个 错误 有点 大 了 。 看看 朝鲜 战争 大事记
 你们 开学 一次性 交 多少 钱 。 # 看 图 不 说话 0906 ： 三星 手机 的 中东 版 。 舆情 监控 ， 已经 是 大陆 媒界 不用 公开 
'''
text.collocations()#最搭配的用词
'''
Displaying 4 of 4 matches:
 北京 市长 准备 如何 收场 ？ 。 # 看 图 不 说话 1016 ： 三星 手机 名不虚传 。 How to mess up # StarWars a
假 论文 ， 那 就 是 丢脸 了 。 # 看 图 不 说话 0923 ： 三星 一生 黑 http://cinacn.blogspot.com/2016/
 ： 心里 想 说 没事 了 。 # 看 图 不 说话 0917 ： 使用 三星 手机 的 后果 。 这个 错误 有点 大 了 。 看看 朝鲜 战争 大事记
 你们 开学 一次性 交 多少 钱 。 # 看 图 不 说话 0906 ： 三星 手机 的 中东 版 。 舆情 监控 ， 已经 是 大陆 媒界 不用 公开 
'''
text.collocations()#最搭配的用词
'''
@Homebaby Wen; little pink; sounds cute; 核废料 处理厂; 诺贝尔 文学奖; 连云港 核废料;
创业者 越来越
'''
fdist = text.vocab()#{词1：词1出现的个数，词2：词2出现的个数，...}
print(fdist["大陆"])
print(fdist["老百姓"])
print(fdist["的"])
print(fdist["不"])
'''
25
27
1250
351
'''
print(len(tokens)) #总共的词数
print(len(fdist.keys())) #不重复词
print(fdist.freq('的'))#1250/27232
print(fdist.most_common(20))# 出现次数最大的前20个
'''
27232
6465
0.04590188014101058
[('，', 1651), ('的', 1250), ('。', 959), ('：', 568), ('是', 495), ('了', 380), ('不', 351), ('”', 257), ('“', 255), ('#', 242), ('一', 222), ('就', 207), ('在', 200), ('有', 162), ('？', 150), ('你', 130), ('我', 129), ('这', 129), ('人', 127), ('都', 124)]
'''
nltk.download('stopwords')
'''
[nltk_data] Downloading package stopwords to /Users/Trucy/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.
然后在解压的停词目录中添加china文件，里面添加停词，这里用的哈工大停词库
'''
comm=list(zip(*fdist.most_common(100)))[0]#100个热词中非停词
print([w for w in comm if w not in nltk.corpus.stopwords.words('china')])
'''
['中国', '说', '国家', '毛', '微', '语录', '精选', '图', '说话', '中共', '日', '政府', '年', '爱国', '美国', '天', '钱', '荟萃', '段子', '里', '想']
'''
# 非url的长词语
print([w for w in fdist.keys() if len(w) > 12 and not w.startswith("http")])
'''
['@jianshenbiao', '@Jackstraw_PRC', '@Andrew19751110', '@boattractor_cj', '@_markhorton搞得后者不得不', '@Weaponmagazine-肖宁）', '@YE5MQ5Vtp2jlWX7', '@Menghuanlangqi2', '@szstupidcool', '@zhifangnvren', 'HappyBirthday', '@freedomandlaw', '@_mackhorton结果攻击了一个字母之差的无辜鬼佬', '@和菜头）——中国人能接受么？丝毫没有对他国的尊重', '@BattlerHenry', '@c338ki_selina', '@shangguanluan', '@zhanghui8964', '@fedsneighbor']
'''
</pre>

相信你肯定听过二八原则，如果把所有的单词（字）放在一起看呢？会不会20%的词（字）占了80%的出现次数？答案是肯定的。

早在上个世纪30年代，就有人（Zipf）对此作出了研究，并给出了量化的表达——齐普夫定律（Zipf’s Law）：一个词在一个有相当长度的语篇中的等级序号（该词在按出现次数排列的词表中的位置，他称之为rank，简称r）与该词的出现次数（他称为frequency，简称f）的乘积几乎是一个常数（constant，简称C）。用公式表示，就是 r × f = C 。（此处的C一般认为取0.1）。Zipf定律是文献计量学的重要定律之一，它和洛特卡定律、布拉德福定律一起被并称为文献计量学的三大定律。

我们直观地验证下，100个热词频度和排名的规律：

<pre>
#前100热词统计
comm=list(zip(*fdist.most_common(400)))
top=[(w,comm[1][i]) for i,w in enumerate(comm[0]) \
if w not in nltk.corpus.stopwords.words('china')]
dd=list(zip(*top[:100]))
plt.plot(range(1,101),dd[1],'-',linewidth=5,alpha=0.6)
plt.xticks(arange(1,101,2),dd[0][::2],rotation=90)
plt.xlabel('前100热词',fontsize='x-large')
plt.ylabel('频数',fontsize='x-large')
plt.title('前100热词统计',fontsize='x-large')
plt.show()

</pre>

![](https://i.imgur.com/TzqnyaM.png)

很明显如果去拟合的话是反比例曲线，有兴趣的使用最小二乘法拟合下，看下c等于多少。同样可以看到，热词中涉及政治词汇比较多，实际上好多中文说说都涉及敏感词汇，这已经是我挑的不怎么敏感的了，万恶腐朽的google+，天朝早晚取缔你😁。
### 15.3.4 特征词提取
这节主要介绍TFIDF，高手直接跳过。

特征词/关键词提取最简单最基础的就是TFIDF，记得5年前我同学让我帮做DI-TFIDF的论文，也就只多了个类内离散度(DI)，今年阿里校招笔试题都有，用mapreduce实现的，很是广泛。实际上tfidf算法的岁数估计比我还大，目前应用还是很广。其他特征提取方法还有TF、MI、ECE、QEMI、IG、OR、GA、SA、PCA、N-Gram等，各有好坏，个人喜欢SA（模拟退火算法）。

简单介绍下TFIDF，TFIDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。TFIDF实际上是：TF * IDF，TF词频(Term Frequency)，IDF反文档频率(Inverse Document Frequency)。TF词频(Term Frequency)指的是某一个给定的词语在该文件中出现的次数）。IDF反文档频率(Inverse Document Frequency)是指果包含词条的文档越少，IDF越大，则说明词条具有很好的类别区分能力。公式看看阮一峰的文章5就懂。

先用波森来玩一下。
<pre>
#第100到109条记录各前五个关键字分析
nlp.extract_keywords(all_content[100:110],top_k=5)
</pre>
结果：

	[[[0.7099039005468386, ‘林志颖’], [0.6618791162771835, ‘说话’], [0.16548951440531323, ‘图’], [0.1391678971188951, ‘真’], [0.08401698377794771, ‘看’]],
	[[0.4023478548923522, ‘面试’], [0.37301990285926334, ‘秋月’], [0.3067035017321436, ‘程序员’], [0.3039517495852464, ‘月饼’], [0.30299584457854634, ‘语录’]],
	[[0.47362866866222336, ‘朝鲜’], [0.3352703984048265, ‘兼听则明’], [0.2612360542645001, ‘志愿军’], [0.25878030672041646, ‘战争’], [0.21926520954291415, ‘脸红’]],
	[[0.49721835098126743, ‘战争’], [0.32820426275864795, ‘共产党’], [0.3033420749749608, ‘朝鲜’], [0.22718247058147462, ‘亲华派’], [0.21472859448533965, ‘保家’]],
	[[0.2454776400796074, ‘老头’], [0.2404142762073897, ‘导游’], [0.18403709160053647, ‘平壤’], [0.1753006540352852, ‘首都’], [0.1462342959779542, ‘坦克’]],
	[[0.611209874316231, ‘语录’], [0.47971562974594106, ‘精选’], [0.3914587127344426, ‘女生’], [0.32629224154255926, ‘复杂’], [0.24636184499215347, ‘到底’]],
	[[0.49698250604754796, ‘响吧’], [0.41302611057797883, ‘老百姓’], [0.34093105823385067, ‘太监’], [0.3321228504052302, ‘大陆’], [0.2893273489670021, ‘税负’]],
	[[0.40325177993397743, ‘导游’], [0.3096728358372618, ‘韩战’], [0.2929205806460173, ‘游客’], [0.2480119777270853, ‘带劲’], [0.22704285072733602, ‘自以为是’]],
	[[0.7242722832662252, ‘哥们儿’], [0.5061338668467491, ‘着急’], [0.4404639544540359, ‘说话’], [0.11012912198474084, ‘图’], [0.07352170001676955, ‘太’]],
	[[0.39101364028081176, ‘鸦片’], [0.27683827573410325, ‘种植’], [0.22542041392575338, ‘zhuo’], [0.22542041392575338, ‘nanjun’], [0.22461629498500293, ‘缩头’]]]

用tfidf来做一下，分四步走：

1. 分词
1. 去停词
1. 计算tfidf
1. 排序，topK

<pre>
cts=[]
#1.十条记录的分词
for c in nlp.tag(all_content[100:110]):
    cts.append(c['word'])
tfidfC=[]
#2.去停词
for ac in cts: 
    tfidfC.append([w for w in ac 
                   if w not in nltk.corpus.stopwords.words('china')])
tc = nltk.TextCollection(tfidfC)
tfidfR=[]
topK=5
#3.计算tfidf
for c in tfidfC:
    res=[]
    for sc in list(set(c)):
        val=tc.tf_idf(sc,c)
        res.append([val,sc])
    #4.topK
    sres=sorted(res, key=lambda p: p[0], reverse=True)[:topK]
    tfidfR.append(sres)
print(tfidfR)
</pre>

结果:
	
	[[[0.4605170185988092, ‘林志颖’], [0.3218875824868201, ‘说话’], [0.3218875824868201, ‘真’], [0.3218875824868201, ‘0913’], [0.3218875824868201, ‘图’]],
	[[0.24237737820989955, ‘面试’], [0.24237737820989955, ‘官’], [0.12118868910494977, ‘从前’], [0.12118868910494977, ‘秋月’], [0.12118868910494977, ‘原因’]],
	[[0.1485538769673578, ‘中’], [0.11651349719283252, ‘朝鲜’], [0.07767566479522169, ‘战争’], [0.0742769384836789, ‘贡献’], [0.0742769384836789, ‘话’]],
	[[0.15703993099903515, ‘战争’], [0.10496334211526741, ‘共产党’], [0.1001123953475672, ‘场’], [0.07851996549951758, ‘朝鲜’], [0.0500561976737836, ‘课本’]],
	[[0.03868841135658895, ‘说’], [0.033210361918183356, ‘健康’], [0.033210361918183356, ‘老头’], [0.033210361918183356, ‘首都’], [0.03095072908527116, ‘抢’]],
	[[0.3837641821656743, ‘关系’], [0.3837641821656743, ‘0912’], [0.3837641821656743, ‘女生’], [0.26823965207235, ‘微’], [0.26823965207235, ‘精选’]], [[0.14631253749400913, ‘大陆’], [0.14631253749400913, ‘老百姓’], [0.10466295877245664, ‘总得’], [0.10466295877245664, ‘倒数’], [0.10466295877245664, ‘毛’]],
	[[0.10233711524417982, ‘想’], [0.07153057388596001, ‘导游’], [0.07153057388596001, ‘说’], [0.07153057388596001, ‘游客’], [0.07153057388596001, ‘年’]],
	[[0.3837641821656743, ‘着急’], [0.3837641821656743, ‘哥们儿’], [0.3837641821656743, ‘0911’], [0.26823965207235, ‘说话’], [0.26823965207235, ‘图’]],
	[[0.1151292546497023, ‘鸦片’], [0.1151292546497023, ‘种植’], [0.05756462732485115, ‘zhuo’], [0.05756462732485115, ‘补习’], [0.05756462732485115, ‘充当’]]]
两者比较下波森要稍好一些，底层估计是改进的tfidf。

### 15.3.5 文本相似度
我们看今日头条新闻，看完后下面会有推荐相似的文章，提供你更多阅读的机会。相似文本推荐最基本的方法用到”余弦相似性“（cosine similiarity）。举个简单例子： 两个文本，先分词：

	文本1：google/和/百度/相似
	文本2：百度/比不了/谷歌

计算词频

	文本1：google:1,和:1,百度:1,相似:1
	文本2：百度:1,比不了:1,谷歌:1

列出所有词，按词频排序：

	文本1：google:1,和:1,百度:1,相似:1,比不了:0
	文本2：google:1,和:0,百度:1,相似:0,比不了:1

提取词频向量：

	文本1：[1,1,1,1,0] 文本2：[1,0,1,0,1]

接下来计算余弦相似度:
![](https://i.imgur.com/Cg5N9Oo.png)
这个公式代表两个向量的夹角，二维向量夹角高中不就是这个公式么，推广到三维甚至多维也一样。

![](https://i.imgur.com/x0yD7Ug.png)

余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，这就叫”余弦相似性”。所以上面两文本相似度为:
![](https://i.imgur.com/Mr35bEc.png)

下面我们来统计下内容较长的记录相似文本(内容较短相似度不明显)。 第一步：找出内容长度大于700的记录，并做好分词和停词。

<pre>
#筛选要比较的文本数据,共24条记录
data=[]#保存object.content内容大于700的所有数据
all_posts=[]#只保留发布的内容数据
for ac in activity_results:
    if len(ac['object']['content'])>700:
        c=ac['object']['content']
        data.append(ac)
        #分词和停词
        all_posts.append([w for w in nlp.tag(c)[0]['word'] if w not in nltk.corpus.stopwords.words('china')])

</pre>
第二步：构造词条文档矩阵。形式如
![](https://i.imgur.com/frDqafp.png)

<pre>
tc = nltk.TextCollection(all_posts)
#构造词条文档矩阵
td_matrix = {}
for idx in range(len(all_posts)):
    post = all_posts[idx]
    fdist = nltk.FreqDist(post)

    doc_title = data[idx]['title']
    url = data[idx]['url']
    td_matrix[(doc_title, url)] = {}

    for term in fdist.keys():
        td_matrix[(doc_title, url)][term] = tc.tf_idf(term, post)
</pre>


第三步:计算相似度

这里用到nltk.cluster.util.cosine_distance，这个函数说明如下： nltk.cluster.util.cosine_distance(u, v)[source] Returns 1 minus the cosine of the angle between vectors v and u. This is equal to 1 - (u.v / |u||v|). 可以看到返回的是1-余弦相似度，表示的是余弦距离，也就是说余弦距离越小相似度越高。

<pre>
distances = {}
for (title1, url1) in td_matrix.keys():

    distances[(title1, url1)] = {}
    (min_dist, most_similar) = (1.0, ('', ''))

    for (title2, url2) in td_matrix.keys():

        terms1 = td_matrix[(title1, url1)].copy()
        terms2 = td_matrix[(title2, url2)].copy()

        # 填充0使两向量长度相同
        for term1 in terms1:
            if term1 not in terms2:
                terms2[term1] = 0

        for term2 in terms2:
            if term2 not in terms1:
                terms1[term2] = 0

        # 产生v1,v2向量，向量元素映射一致
        v1 = [score for (term, score) in sorted(terms1.items())]
        v2 = [score for (term, score) in sorted(terms2.items())]

        # 计算余弦相似度
        distances[(title1, url1)][(title2, url2)] = \
            nltk.cluster.util.cosine_distance(v1, v2)

        if url1 == url2:
            continue
        # 标记余弦距离最小的（相似度最大）
        if distances[(title1, url1)][(title2, url2)] < min_dist:
            (min_dist, most_similar) = (distances[(title1, url1)][(title2,
                                         url2)], (title2, url2))
    
    print ("%s\n(%s) \n 最相似的是:\n %s\n(%s) \n 相似度: %f \n ------------------------------\n"
           % (title1, url1,most_similar[0], most_similar[1], 1-min_dist))
</pre>

查看部分结果：为查看方便，绘制相似度矩阵图：
![](https://i.imgur.com/0M0j8MA.png)


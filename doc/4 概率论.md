# <center> 第 4 章 概率论 </center>

> 导读：机器学习与深度学习是一门多学科交叉的科学技术，其中数学尤为重要，很多形式化模型向数学建模的必经过程。继上章线性代数核心知识的介绍之后，本章着重介绍概率论相关知识。由于基于规则方法向基于统计方法的转型，概率就显得尤为重要，诸如一些随机事件、独立假设、条件概率、完全概率等等。然后对贝叶斯模型进行案例式介绍，旨在读者深度理解。最后，我们将信息论归为本章，重点介绍信息论相关概念和信息度量的常用方法。
##	4.1 概率论介绍
> 概率论概述

概率论（英语：Probability theory）是集中研究概率及随机现象的数学分支，是研究随机性或不确定性等现象的数学。概率论主要研究对象为随机事件、随机变量以及随机过程。对于随机事件是不可能准确预测其结果的，然而对于一系列的独立随机事件——例如掷骰子、扔硬币、抽扑克牌以及轮盘等，会呈现出一定的、可以被用于研究及预测的规律，两个用来描述这些规律的最具代表性的数学结论分别是大数定律和中心极限定理。

> 概率的生活案例之六合彩

买5, 17, 19, 24, 33, 49中奖概率高还是买1,2,3,4,5,6的中奖概率高?

古典概率论说：一样。但实际上机械或彩球制造上都有些微小的差异，所以每组概率不一定完全相同，但必须累积多期开奖结果后才看得出来。

> 概率的生活案例之生日悖论

在一个足球场上有23个人（2×11个运动员和1个裁判员），不可思议的是，在这23人当中至少有两个人的生日是在同一天的概率要大于50％。 如果这23人都没有相同的生日也不违反概率，只是小于50％。

> 概率的生活案例之轮盘游戏

在游戏中玩家可能认为，在连续出现多次红色后，出现黑色的概率会越来越大。
这种判断也是错误的，即出现黑色的概率每次是相等的，因为球本身并没有“记忆”， 它不会意识到以前都发生了什么，其概率始终是 18/37。但轮盘的前后期开奖数字形成时间序列（可能存在自回归模型）。

>  概率的生活案例之赢取名车

赢取电视节目里的名车：在参赛者面前有三扇关闭的门，其中只有一扇后面有名车，而其余的后面是山羊。

游戏规则是，参赛者先选取一扇门，但在他打开之前，主持人在其余两扇门中打开了一扇有山羊的门， 并询问参赛者是否改变主意选择另一扇门，以使赢得名车的概率变大。

正确的分析结果是，假如不管开始哪一扇门被选，主持人都打开其余两扇门中有山羊的那一扇并询问参赛者是否改变主意， 则改变主意会使赢得汽车的概率增加一倍；（“标准”的三门问题情况。）

假如主持人只在有名车那扇门被选中时劝诱参赛者打开其它门，则改变主意必输。(资讯不对称)


##	4.2 事件
### 4.2.1 随机试验
> 随机试验的定义

我们将对自然现象的一次观察或进行一次科学试验称为试验。

>  随机试验的例子

举例1:硬币试验

- E1: 抛一枚硬币，观察正(H)反(T) 面的情况。
- E2: 将一枚硬币抛三次,观察正反面出现的情况。
- E3: 将一枚硬币抛三次，观察出现正面的情况。
- E4: 电话交换台一分钟内接到的呼唤次数。
- E5: 在一批灯泡中任取一只, 测试它的寿命。

举例2:数学家去赌场

新闻：数学家3年赌赢156亿人民币，数学家在赌场里有什么优势？

令19名数学家惊喜的是，虽然他们所掌握的那些高深数学知识在现实生活中似乎派不上多大用场，但竟然出人意料地在赌场上显现出了巨大的威力！据悉，19名数学家参与的大多是赛马、赛狗以及21点之类的赌博项目。而每次下注之前，他们会利用自己所精通的专业数学方法对各种中奖的概率进行推理演算，从而研究出某种“逢赌必赢”的秘笈！因为它的形态看起来合乎理想。在现实生活中，遇到测量之类的大量连续数据时，你“正常情况下”会期望看到这种形态。

### 4.2.2 随机事件和样本空间

> 基本事件或单位事件

定义：在一次随机试验中可能发生的不能再细分的结果被称为基本事件，或者称为单位事件，用 <img src="http://latex.codecogs.com/gif.latex?{\displaystyle E}" /> 表示。

> 样本空间

定义：在随机试验中可能发生的所有单位事件的集合称为事件空间，用 <img src="http://latex.codecogs.com/gif.latex?{\displaystyle S}" />来表示。

例如：在一次掷骰子的随机试验中，如果用获得的点数来表示单位事件，那么一共可能出现 6 个单位事件，则事件空间可以表示为<img src="http://latex.codecogs.com/gif.latex?{\displaystyle S=\{1,2,3,4,5,6\}}" /> 。

上面的事件空间是由可数有限单位事件组成，事实上还存在着由可数无限以及不可数单位事件组成的事件空间，比如在一次获得正面朝上就停止的随机掷硬币试验中，其事件空间由可数无限单位事件组成，表示为： S={正，反正，反反正，反反反正，反反反反正，···}，注意到在这个例子中"反反反正"是单位事件。将两根筷子随意扔向桌面，其静止后所形成的交角假设为<img src="http://latex.codecogs.com/gif.latex?{\displaystyle \alpha }" /> ，这个随机试验的事件空间的组成可以表示为 <img src="http://latex.codecogs.com/gif.latex?{\displaystyle S=\{\alpha |0^{\circ }\leq \alpha <180^{\circ }\}}" /> 。

> 随机事件

随机事件是事件空间<img src="http://latex.codecogs.com/gif.latex?{\displaystyle S }" />的子集，它由事件空间 <img src="http://latex.codecogs.com/gif.latex?{\displaystyle S }" /> 中的单位元素构成，用大写字母<img src="http://latex.codecogs.com/gif.latex? {\displaystyle A,B,C\cdots } " /> 表示。例如在掷两个骰子的随机试验中，设随机事件 <img src="http://latex.codecogs.com/gif.latex? {\displaystyle A} " /> = “获得的点数和大于10”，则 <img src="http://latex.codecogs.com/gif.latex? {\displaystyle A} " /> 可以由下面 3 个单位事件组成：<img src="http://latex.codecogs.com/gif.latex?{\displaystyle A=\{(5,6),(6,5),(6,6)\}} " /> 。

> 必然事件和不可能事件

如果在随机试验中事件空间中的所有可能的单位事件都发生，这个事件被称为 必然事件；相应的如果事件空间里不包含任何一个单位事件，则称为不可能事件。

### 4.2.3 事件的计算

因为事件在一定程度上是以集合的含义定义的，因此可以把集合计算方法直接应用于事件的计算，也就是说，在计算过程中，可以把事件当作集合来对待，如图4-1所示。
<center>
![](https://i.imgur.com/7tKkS5F.png)

图4-1 集合计算
</center>
在轮盘游戏中假设 <img src="http://latex.codecogs.com/gif.latex? {\displaystyle A} " />代表事件“球落在红色区域”，<img src="http://latex.codecogs.com/gif.latex? {\displaystyle B} " />代表事件"球落在黑色区域"，因为事件 <img src="http://latex.codecogs.com/gif.latex? {\displaystyle A} " />和<img src="http://latex.codecogs.com/gif.latex? {\displaystyle B} " />没有共同的单位事件，因此可表示为
<img src="http://latex.codecogs.com/gif.latex? {\displaystyle A\cap B=\varnothing }" />
注意到事件<img src="http://latex.codecogs.com/gif.latex? {\displaystyle A} " /> 和 <img src="http://latex.codecogs.com/gif.latex? {\displaystyle B} " /> 并不是互补的关系，因为在整个事件空间 <img src="http://latex.codecogs.com/gif.latex? {\displaystyle S} " /> 中还有一个单位事件“零”，其即不是红色也不是黑色，而是绿色。

##	4.3 概率

> 古典概率

古典概率又叫传统概率或拉普拉斯概率，古典概率的定义是由法国数学家拉普拉斯 ( Laplace ) 提出的。如果一个随机试验所包含的单位事件是有限的，且每个单位事件发生的可能性均相等，则这个随机试验叫做拉普拉斯试验。在拉普拉斯试验中，事件 <img src="http://latex.codecogs.com/gif.latex? {\displaystyle A} " />在事件空间 <img src="http://latex.codecogs.com/gif.latex? {\displaystyle S} " />中的概率<img src="http://latex.codecogs.com/gif.latex? {\displaystyle P(A)} " /> 为：

P(A)=构成事件A的元素数目/构成事件空间S的所有元素数目

例如，在一次同时掷一个硬币和一个骰子的随机试验中，假设事件 A 为获得国徽面且点数大于 4 ，那么事件 A 的概率应该有如下计算方法：  S= { ( 国徽，1 点 )，( 数字，1 点 )，( 国徽，2 点 )，( 数字，2 点 )，( 国徽，3 点 )，( 数字，3 点 )，( 国徽，4 点 )，( 数字，4 点 )，( 国徽，5 点 )，( 数字，5 点 )，( 国徽，6 点 )，( 数字，6 点 ) }， A＝{( 国徽，5 点 )，( 国徽，6 点 )}，按照拉普拉斯定义，  A 的概率为，

<img src="http://latex.codecogs.com/gif.latex? {\displaystyle P(A)={\frac {2}{12}}={\frac {1}{6}}}" />


注意到在拉普拉斯试验中存在着若干的疑问，在现实中是否存在着其单位事件的概率具有精确相同的概率值的试验? 因为我们不知道，硬币以及骰子是否完美，即骰子制造的是否均匀，其重心是否位于正中心，以及轮盘是否倾向于某一个数字。 尽管如此，传统概率在实践中被广泛应用于确定事件的概率值，其理论根据是： 如果没有足够的论据来证明一个事件的概率大于另一个事件的概率，那么可以认为这两个事件的概率值相等。

如果仔细观察这个定义会发现拉普拉斯用概率解释了概率，定义中用了相同的可能性 ( 原文是 également possible )一词，其实指的就是"相同的概率"。这个定义也并没有说出，到底什么是概率，以及如何用数字来确定概率。在现实生活中也有一系列问题，无论如何不能用传统概率定义来解释，比如，人寿保险公司无法确定一个 50 岁的人在下一年将死去的概率。

> 古典概率的两个特点

1. 样本空间的元素只有有限个。
1. 实验中每个基本事件发生的可能性相同。

> 统计概率：大数定律

继传统概率论之后，英国逻辑学家约翰·维恩和奥地利数学家理查德提出建立在频率理论基础上的统计概率。他们认为，获得一个事件的概率值的唯一方法是通过对该事件进行 100 次，1000 次或者甚至 10000 次的前后相互独立的  n 次随机试验，针对每次试验均记录下绝对频率值  <img src="http://latex.codecogs.com/gif.latex? h_{n}(A)" />和相对频率值<img src="http://latex.codecogs.com/gif.latex?  {\displaystyle f_{n}}" />，随着试验次数 n 的增加，会出现如下事实，即相对频率值会趋于稳定，它在一个特定的值上下浮动，也即是说存在着一个极限值  P(A)，相对频率值趋向于这个极限值。这个极限值被称为统计概率，表示为：
<img src="http://latex.codecogs.com/gif.latex?  {\displaystyle P(A)=\lim _{n\to \infty }f_{n}(A)}" />

例如，若想知道在一次掷骰子的随机试验中获得 6 点的概率值可以对其进行 3000 次前后独立的扔掷试验（如表4-1所示），在每一次试验后记录下出现 6 点的次数，然后通过计算相对频率值可以得到趋向于某一个数的统计概率值。
<center>

表4-1 掷骰子随机试验统计结果
<table>
<tbody valign="top" style="color: black; font-family: 宋体; font-size: 10pt;">
	<tr style="background: #5b9bd5; text-align:center;vertical-align: middle;font-weight: bold;">
	<td >扔掷数</td>
	<td >获得6点的绝对频率</td>
	<td >获得6点的相对频率</td>
	</tr>

	<tr style=" background: #deeaf6;">
	<td>1</td>
	<td>1</td>
	<td>1.00000</td>
	</tr>

	<tr >
	<td>2</td>
	<td >1</td>
	<td >0.50000</td>
	</tr>

	<tr style=" background: #deeaf6;">
	<td>3</td>
	<td>1</td>
	<td>0.33333</td>
	</tr>

	<tr >
	<td>4</td>
	<td >1</td>
	<td >0.25000</td>
	</tr>

	<tr style=" background: #deeaf6;">
	<td>5</td>
	<td>2</td>
	<td>0.40000</td>
	</tr>

	<tr >
	<td>10</td>
	<td >2</td>
	<td >0.20000</td>
	</tr>

	<tr style=" background: #deeaf6;">
	<td>20</td>
	<td>5</td>
	<td>0.25000</td>
	</tr>

	<tr >
	<td>100</td>
	<td >12</td>
	<td >0.12000</td>
	</tr>

	<tr style=" background: #deeaf6;">
	<td>200</td>
	<td>39</td>
	<td>0.19500</td>
	</tr>

	<tr >
	<td>300</td>
	<td >46</td>
	<td >0.15333</td>
	</tr>

	<tr style=" background: #deeaf6;">
	<td>400</td>
	<td>72</td>
	<td>0.18000</td>
	</tr>

	<tr >
	<td>500</td>
	<td >76</td>
	<td >0.15200</td>
	</tr>

	<tr style=" background: #deeaf6;">
	<td>1000</td>
	<td>170</td>
	<td>0.17000</td>
	</tr>

	<tr >
	<td>2000</td>
	<td >343</td>
	<td >0.17150</td>
	</tr>

	<tr style=" background: #deeaf6;">
	<td>3000</td>
	<td>506</td>
	<td>0.16867</td>
	</tr>

</tbody>
</table>


</center>
上面提到的这个有关相对频率的经验规律是大数定律在现实生活中的反映，大数定律是初等概率论的基础。统计概率在今天的实践中依然具有重要意义，特别是在初等概率论及数理统计等学科中。
##	4.4 概率公理

> 公理 1：事件  A 的概率  P(A) 是一个0与1之间（包含0与1）的非负实数。
<p>
<img src="http://latex.codecogs.com/gif.latex?  {\displaystyle 0\leq P(A)\leq 1\ (A\in S)}" />
</p>


> 公理 2：  事件空间的概率值为 1 。
<p><img src="http://latex.codecogs.com/gif.latex?  {\displaystyle P(S)=1}" /></p>


> 公理 3： ， 互斥事件的加法法则。这里需注意：公理3可以推广到可数个互斥事件的联集。
<p><img src="http://latex.codecogs.com/gif.latex?  {\displaystyle P(A\cup B)=P(A)+P(B)} \if {\displaystyle A\cap B=\varnothing }" /></p>


> 定理 1 (互补法则)：与 A 互补事件的概率始终是

<p><img src="http://latex.codecogs.com/gif.latex?  {\displaystyle P({\bar {A}})=1-P(A),\in S}" /></p>

> 定理 2：不可能事件的概率为零：

<p><img src="http://latex.codecogs.com/gif.latex?  {\displaystyle P(\varnothing )=0} " /></p>

> 定理 3：如果若干事件 <img src="http://latex.codecogs.com/gif.latex? {\displaystyle A_{1},A_{2},\cdots A_{n}\in S} " /> 每两两之间是空集关系，那么这些所有事件集合的概率等于单个事件的概率的和。

<p><img src="http://latex.codecogs.com/gif.latex?  {\displaystyle P(A_{1}\cup \cdots \cup A_{n})=\sum _{j=1}^{n}P(A_{j})} " /></p>


注意针对这一定理有效性的决定因素是<img src="http://latex.codecogs.com/gif.latex?  {\displaystyle A_{1}\cdots A_{n}}" /> 事件不能同时发生。例如，在一次掷骰子中，得到 5 点或者 6 点的概率是：

<p><img src="http://latex.codecogs.com/gif.latex?  {\displaystyle P=P(A_{5})+P(A_{6})=} {\displaystyle ={\frac {2}{6}}={\frac {1}{3}}} " /></p>

> 定理 4：如果事件 A，B 是差集关系，则有，

<p><img src="http://latex.codecogs.com/gif.latex?  {\displaystyle P(A\setminus B)=P(A)-P(A\cap B)}" /></p>


> 定理 5 (任意事件加法法则)：对于事件空间S 中的任意两个事件 A 和 B，有如下定理：

<p><img src="http://latex.codecogs.com/gif.latex?  {\displaystyle P(A\cup B)=P(A)+P(B)-P(A\cap B)} " /></p>

例如，在由一共 32 张牌构成的斯卡特扑克牌中随机抽出一张，其或者是"方片"或者是" <img src="http://latex.codecogs.com/gif.latex?{\mathcal {A}}} " /> "的概率是多少？

事件 A， B 是或者的关系，且可同时发生，就是说抽出的这张牌即可以是"方片"，又可以是" <img src="http://latex.codecogs.com/gif.latex?{\mathcal {A}}} " /> "， A ∩ B ( 既发生  A 又发生 B ) 的值是 1 / 32，因此有如下结果：
<p><img src="http://latex.codecogs.com/gif.latex?  {\displaystyle P(A\cup B)={\frac {8}{32}}+{\frac {4}{32}}-{\frac {1}{32}}={\frac {11}{32}}}" /></p>


> 定理 6 (乘法法则)：事件 A， B 同时发生的概率是：
<p><img src="http://latex.codecogs.com/gif.latex?  {\displaystyle P(A\cap B)=P(A)\cdot P(B\vert A)=P(B)\cdot P(A\vert B)}" /></p>

公式中的  P(A|B) 是指在 B 条件下  A 发生的概率，又称作条件概率。回到上面的斯卡特游戏中，在 32 张牌中随机抽出一张，即是方片又是  <img src="http://latex.codecogs.com/gif.latex?{\mathcal {A}}} " /> 的概率是多少呢？现用  P(A) 代表抽出方片的概率，用P(B) 代表抽出 <img src="http://latex.codecogs.com/gif.latex?{\mathcal {A}}} " /> 的概率，很明显， A， B 之间有一定联系，即 A 里包含有 B，  B 里又包含有 {\displaystyle A} A，在 A 的条件下发生 B 的概率是 P(B | A)=1/8，则有：
<p><img src="http://latex.codecogs.com/gif.latex?  {\displaystyle P(A\cap B)=P(A)\cdot P(B\vert A)={\frac {8}{32}}\cdot {\frac {1}{8}}={\frac {1}{32}}} " /></p>


或者，
<p><img src="http://latex.codecogs.com/gif.latex?   {\displaystyle P(A\cap B)=P(B)\cdot P(A\vert B)={\frac {4}{32}}\cdot {\frac {1}{4}}={\frac {1}{32}}} " /></p>

 
从上面的图中也可以看出，符合条件的只有一张牌，即方片  <img src="http://latex.codecogs.com/gif.latex?{\mathcal {A}}} " /> 。
另一个例子，在 32 张斯卡特牌里连续抽两张 ( 第一次抽出的牌不放回去 )，连续得到两个 <img src="http://latex.codecogs.com/gif.latex?{\mathcal {A}}} " />的概率是多少呢？

设 A，  B 分别为连续发生的这两次事件，我们看到，  A， B 之间有一定联系，即  B 的概率由于  A 发生了变化，属于条件概率，按照公式有： 

<p><img src="http://latex.codecogs.com/gif.latex?  {\displaystyle P(A\cap B)=P(A)\cdot P(B\vert A)={\frac {4}{32}}\cdot {\frac {3}{31}}={\frac {3}{248}}} " /></p>


> 定理 7 (无关事件乘法法则)：两个不相关联的事件 A， B 同时发生的概率是：

<p><img src="http://latex.codecogs.com/gif.latex?  {\displaystyle P(A\cap B)=P(A)\cdot P(B)}" /></p>

注意到这个定理实际上是定理 6 (乘法法则) 的特殊情况，如果事件  A，  B 没有联系，则有 P(A|B)=P(A)，以及 P(B|A)=P(B)。现在观察一下轮盘游戏中两次连续的旋转过程， P(A) 代表第一次出现红色的概率，P(B) 代表第二次出现红色的概率，可以看出，  A 与  B 没有关联，利用上面提到的公式，连续两次出现红色的概率为：

<p><img src="http://latex.codecogs.com/gif.latex?  {\displaystyle P(A\cap B)={\frac {18}{37}}\cdot {\frac {18}{37}}=0.2367}" /></p>

 

##	4.5 条件概率和全概率 
### 4.5.1 条件概率
> 条件概率的描述

设试验E的样本空间为S, 事件包括A,B, 要考虑在A已经发生的条件下B发生的概率, 这就是条件概率问题。

> 条件概率的定义

设A，B是两个事件，且P(A)>0，称： <img src="http://latex.codecogs.com/gif.latex?  P(A|B)=\frac{P(AB)}{P(A)}" /> （AB不独立）  

设A，B是两个事件，且P(A)>0，称：  <img src="http://latex.codecogs.com/gif.latex?  P(A|B)=P(A)" /> （AB独立）          

> 条件概率的性质

性质1：对于每一个事件B，有： <img src="http://latex.codecogs.com/gif.latex?  0 \leqslant P(B|A)\leqslant 1" />                      

性质2：<img src="http://latex.codecogs.com/gif.latex? P(S|A)=1" />   

性质3. 设<img src="http://latex.codecogs.com/gif.latex? B_1,B_2,...,B_n" />两两互不相容，则 <img src="http://latex.codecogs.com/gif.latex? P(UB_i|A)=\sum P(B_i|A)" /> 
                      
> 条件概率的计算方法

（1）公式法：

先计算P(A)，P(AB)，然后按公式计算P(B|A)=P(AB)/P(A)

（2）图解法：利用概率树求解

例如： 图圈饼店正在调查客户购买圈饼和咖啡的概率，下面是一些线索，画出概率树并求解相应概率（如图4-2所示）。以下是已知条件：

- P(圈饼) = 3/4  
- P(咖啡|圈饼’) = 1/3
- P(圈饼∩咖啡) = 9/20

<center>

![](https://i.imgur.com/tDXcs5w.png)

图4-2 圈饼概率树

</center>
计算过程：

P(咖啡|圈饼) = P(圈饼∩咖啡) / P(圈饼) = 3/5

P(咖啡|圈饼`) = P(圈饼`∩咖啡) / P(圈饼`) = 1/3

P(咖啡`|圈饼) = P(圈饼∩咖啡`) / P(圈饼) = 2/5

P(咖啡`|圈饼`) = P(圈饼`∩咖啡`) / P(圈饼`) = 2/3

> 使用概率树求解问题的优缺点：

- 优点： 能够以图形体现条件概率，同时帮助计算概率，利用分支结构，条理清楚，不易算错。
- 不足： 画概率树很浪费时间。

### 4.5.2 完全概率
> 概念介绍


n 个事件<img src="http://latex.codecogs.com/gif.latex?  {\displaystyle H_{1},H_{2},...H_{n}}" /> 互相间独立，且共同组成整个事件空间 S，即

<p><img src="http://latex.codecogs.com/gif.latex?  
{\displaystyle H_{i}\cap H_{j}=\varnothing } , {\displaystyle (i\neq j)}" /></p>

 以及

<p><img src="http://latex.codecogs.com/gif.latex?  
{\displaystyle H_{1}\cup H_{2}\cup ...\cup H_{n}=S}" /></p>

这时  A 的概率可以表示为，
<p><img src="http://latex.codecogs.com/gif.latex?  
{\displaystyle P(A)=\sum _{j=1}^{n}P(A|H_{j})\cdot P(H_{j})}" /></p>

>举例解析

例如，一个随机试验工具由一个骰子和一个柜子中的三个抽屉组成，抽屉 1 里有 14 个白球和 6 个黑球，抽屉 2 里有 2 个白球和 8 个黑球，抽屉 3 里有 3 个白球和 7 个黑球，试验规则是首先掷骰子，如果获得小于 4 点，则抽屉 1 被选择，如果获得 4 点或者 5 点，则抽屉 2 被选择，其他情况选择抽屉 3 。然后在选择的抽屉里随机抽出一个球，最后抽出的这个球是白球的概率是：

P(白)=P(白|抽1)·P(抽1)+P(白|抽2)·P(抽2)＋P(白|抽3)·P(抽3)

=(14/20)·(3/6)+(2/10)·(2/6)+(3/10)·(1/6)

=28/60=0.4667

从例子中可看出，完全概率特别适合于分析具有多层结构的随机试验的情况。

##	4.6 贝叶斯定理

> 贝叶斯公式

贝叶斯定理由英国数学家托马斯·贝叶斯 ( Thomas Bayes 1702-1761 ) 发展，用来描述两个条件概率之间的关系，比如 P(A|B) 和 P(B|A)。按照定理 6 的乘法法则，P(A∩B)=P(A)·P(B|A)=P(B)·P(A|B)，可以立刻导出贝叶斯定理：

<p><img src="http://latex.codecogs.com/gif.latex?  
{\displaystyle P(A\vert B)={\frac {P(B\vert A)\cdot P(A)}{P(B)}}}" /></p>




> 案例1：狗叫抓贼

例如：一座别墅在过去的 20 年里一共发生过 2 次被盗，别墅的主人有一条狗，狗平均每周晚上叫 3 次，在盗贼入侵时狗叫的概率被估计为 0.9，问题是：在狗叫的时候发生入侵的概率是多少？

我们假设 A 事件为狗在晚上叫， B 为盗贼入侵，则

<p><img src="http://latex.codecogs.com/gif.latex?  
 P(A)=3/7,P(B)=2/(20·365.25)=2/7305,P(A | B) = 0.9" /></p>
按照公式很容易得出结果：
<p><img src="http://latex.codecogs.com/gif.latex?  
{\displaystyle P(B\vert A)=0.9\cdot {\frac {2}{7305}}\cdot {\frac {7}{3}}=0.0005749486653...}" /></p>

> 案例2：追踪红球

现分别有  A，  B 两个容器，在容器  A 里分别有 7 个红球和 3 个白球，在容器  B 里有 1 个红球和 9 个白球，现已知从这两个容器里任意抽出了一个球，且是红球，问这个红球是来自容器  A 的概率是多少?

假设已经抽出红球为事件 B，从容器 A 里抽出球为事件  A，则有：  
<p><img src="http://latex.codecogs.com/gif.latex?  
 P(B) = 8 / 20, P(A) = 1 / 2,  P(B|A) = 7 / 10" /></p>
按照公式，则有：
<p><img src="http://latex.codecogs.com/gif.latex?  
{\displaystyle P(A\vert B)={\frac {7}{10}}\cdot {\frac {1}{2}}\cdot {\frac {20}{8}}={\frac {7}{8}}}" /></p>


##	4.7 信息论

### 4.7.1 信息论的基本概念
> 信息论


信息论（英语：information theory）是应用数学、电机工程学和计算机科学的一个分支，涉及信息的量化、存储和通信等。信息论是由香农发展，用来找出信号处理与通信操作的基本限制，如数据压缩、可靠的存储和数据传输等。自创立以来，它已拓展应用到许多其他领域，包括统计推断、自然语言处理、密码学、神经生物学、进化论和分子编码的功能、生态学的模式选择、热物理、量子计算、语言学、剽窃检测、模式识别、异常检测和其他形式的数据分析。

> 信息熵

熵是信息的一个关键度量，通常用一条消息中需要存储或传输一个符号的平均比特数来表示。熵衡量了预测随机变量的值时涉及到的不确定度的量。例如，指定掷硬币的结果（两个等可能的结果）比指定掷骰子的结果（六个等可能的结果）所提供的信息量更少（熵更少）。

1948年，香农引入信息熵，将其定义为离散随机事件的出现概率。一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高。所以说，信息熵可以被认为是系统有序化程度的一个度量。

### 4.7.2 信息度量
> 信息熵

如果一个随机变量X的可能取值为<img src="http://latex.codecogs.com/gif.latex?X=\left\{ x_{1},x_{2} ,.....,x_{n}   \right\}" /> ，其概率分布为<img src="http://latex.codecogs.com/gif.latex?P\left( X=x_{i}  \right) =p_{i} ,i=1,2,.....,n" /> ，则随机变量X的熵定义为H(X)：

<img src="http://latex.codecogs.com/gif.latex?H\left( X \right) =-\sum_{i=1}^{n}{P\left( x_{i}  \right) logP\left( x_{i}  \right) } =\sum_{i=1}^{n}{P\left( x_{i}  \right) \frac{1}{logP\left( x_{i}  \right) } } " />

其中 x是定义在 X上的随机变量。信息熵是随机事件不确定性的度量。

例子：若S为一个三个面的骰子，

- P(面一)=1/5
- P(面二)=2/5
- P(面三)=2/5

<p><img src="http://latex.codecogs.com/gif.latex?  
{\displaystyle H(X)={\frac {1}{5}}\log _{2}(5)+{\frac {2}{5}}\log _{2}\left({\frac {5}{2}}\right)+{\frac {2}{5}}\log _{2}\left({\frac {5}{2}}\right)} " /></p>


> 联合熵

两个随机变量X和Y的联合分布可以形成联合熵，定义为联合自信息的数学期望，它是二维随机变量XY的不确定性的度量，用H(X,Y)表示：

<p><img src="http://latex.codecogs.com/gif.latex?  
H\left( X,Y \right) =-\sum_{i=1}^{n}{\sum_{j=1}^{n}{P\left( x_{i} ,y_{j}  \right)} logP\left( x_{i},y_{j}   \right)  } " /></p>

 

> 条件熵

在随机变量X发生的前提下，随机变量Y发生新带来的熵，定义为Y的条件熵，用H(Y|X)表示：

<p><img src="http://latex.codecogs.com/gif.latex?  
H\left(Y|X \right) =-\sum_{x,y}{P\left( x,y \right) logP\left( y|x \right) }" /></p>

 
条件熵用来衡量在已知随机变量X的条件下，随机变量Y的不确定性。

由贝氏定理，我们有<img src="http://latex.codecogs.com/gif.latex?{\displaystyle p(x,y)=p(y|x)p(x)}" />  ，代入联合熵的定义，可以分离出条件熵，于是得到联合熵与条件熵的关系式：

<p><img src="http://latex.codecogs.com/gif.latex?  
H\left( Y|X \right) =H\left( X,Y\right) -H\left( X \right)" /></p>

 
推导过程如下：

![](https://i.imgur.com/fiZVEEu.jpg)

上式中：

- 第二行推到第三行的依据是边缘分布P(x)等于联合分布P(x,y)的和；
- 第三行推到第四行的依据是把公因子logP(x)乘进去，然后把x,y写在一起；
- 第四行推到第五行的依据是：因为两个sigma都有P(x,y)，故提取公因子P(x,y)放到外边，然后把里边的-（log P(x,y) log P(x)）写成log (P(x,y) / P(x) ) ；
- 第五行推到第六行的依据是：P(x,y) = P(x) * P(y|x)，故P(x,y) / P(x) = P(y|x)。

> 相对熵：信息增益

相对熵又称互熵、交叉熵、KL散度、信息增益，是描述两个概率分布P和Q差异的一种方法，记为D(P||Q)。在信息论中，D(P||Q)表示当用概率分布Q来拟合真实分布P时，产生的信息损耗，其中P表示真实分布，Q表示P的拟合分布。

对于一个离散随机变量的两个概率分布P和Q来说，它们的相对熵定义为：

<p><img src="http://latex.codecogs.com/gif.latex?  
D\left( P||Q \right) =\sum_{i=1}^{n}{P\left( x_{i}  \right) log\frac{P\left( x_{i}  \right) }{Q\left( x_{i}  \right) } } " /></p>


注意：D(P||Q) ≠ D(Q||P)

> 互信息

两个随机变量X，Y的互信息定义为X，Y的联合分布和各自独立分布乘积的相对熵称为互信息，用I(X,Y)表示。互信息是信息论里一种有用的信息度量方式，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。互信息是两个事件集合之间的相关性。

<p><img src="http://latex.codecogs.com/gif.latex?I\left( X,Y \right) =\sum_{x\in X}{\sum_{y\in Y}{P\left( x,y \right) } log\frac{P\left( x,y \right) }{P\left( x \right) P\left( y \right) } } " /></p>
 
互信息、熵和条件熵之间存在以下关系：

<p><img src="http://latex.codecogs.com/gif.latex? H\left( Y|X \right) =H\left( Y \right) -I\left( X,Y \right)  " /></p>



推导过程如下：

![](https://i.imgur.com/w5Y8TUg.jpg)

通过上面的计算过程发现有：H(Y|X) = H(Y) - I(X,Y)，又由前面条件熵的定义有：H(Y|X) = H(X,Y) - H(X)，于是有I(X,Y)= H(X) + H(Y) - H(X,Y)，此结论被多数文献作为互信息的定义。

> 最大熵

最大熵原理是概率模型学习的一个准则，它认为：学习概率模型时，在所有可能的概率分布中，熵最大的模型是最好的模型。通常用约束条件来确定模型的集合，所以，最大熵模型原理也可以表述为：在满足约束条件的模型集合中选取熵最大的模型。

前面我们知道，若随机变量X的概率分布是<img src="http://latex.codecogs.com/gif.latex?P\left( x_{i}  \right)" /> ，则其熵定义如下：

<img src="http://latex.codecogs.com/gif.latex?H\left( X \right) =-\sum_{i=1}^{n}{P\left( x_{i}  \right) logP\left( x_{i}  \right) } =\sum_{i=1}^{n}{P\left( x_{i}  \right) \frac{1}{logP\left( x_{i}  \right) } } " /> 


熵满足下列不等式：

<img src="http://latex.codecogs.com/gif.latex?0\leq H\left( X \right) \leq log\left| X \right| " /> 


式中，|X|是X的取值个数，当且仅当X的分布是均匀分布时右边的等号成立。也就是说，当X服从均匀分布时，熵最大。

直观地看，最大熵原理认为：要选择概率模型，首先必须满足已有的事实，即约束条件；在没有更多信息的情况下，那些不确定的部分都是“等可能的”。最大熵原理通过熵的最大化来表示等可能性；“等可能”不易操作，而熵则是一个可优化的指标。


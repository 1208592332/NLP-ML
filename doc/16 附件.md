<center style="font:30px Tahoma,Helvetica,Arial,'宋体',sans-serif;"> 第四部分 扩展部分 </center>
# <center> 附A 中英术语纵览 </center>

<B> Basis(基础)</B>：

- SSE(Sum of Squared Error, 平方误差和)
- SAE(Sum of Absolute Error, 绝对误差和)
- SRE(Sum of Relative Error, 相对误差和)
- MSE(Mean Squared Error, 均方误差)
- RMSE(Root Mean Squared Error, 均方根误差)
- RRSE(Root Relative Squared Error, 相对平方根误差)
- MAE(Mean Absolute Error, 平均绝对误差)
- RAE(Root Absolute Error, 平均绝对误差平方根)
- MRSE(Mean Relative Square Error, 相对平均误差)
- RRSE(Root Relative Squared Error, 相对平方根误差)
- Expectation(期望)&Variance(方差)
- Standard Deviation(标准差，也称Root Mean Squared Error, 均方根误差)
- CP(Conditional Probability, 条件概率)
- JP(Joint Probability, 联合概率)
- MP(Marginal Probability, 边缘概率)
- Bayesian Formula(贝叶斯公式)
- CC(Correlation Coefficient, 相关系数)
- Quantile (分位数)
- Covariance(协方差矩阵)
- GD(Gradient Descent, 梯度下降)
- SGD(Stochastic Gradient Descent, 随机梯度下降)
- LMS(Least Mean Squared, 最小均方)
- LSM(Least Square Methods, 最小二乘法)
- NE(Normal Equation, 正规方程)
- MLE(Maximum Likelihood Estimation, 极大似然估计)
- QP(Quadratic Programming, 二次规划)
- L1 /L2 Regularization(L1/L2正则, 以及更多的, 现在比较火的L2.5正则等)
- Eigenvalue(特征值)
- Eigenvector(特征向量)

<B> Common Distribution(常见分布)</B>：

> Discrete Distribution(离散型分布)：

- Bernoulli Distribution/Binomial Distribution(贝努利分布/二项分布)
- Negative Binomial Distribution(负二项分布)
- Multinomial Distribution(多项分布)
- Geometric Distribution(几何分布)
- Hypergeometric Distribution(超几何分布)
- Poisson Distribution (泊松分布)

> Continuous Distribution (连续型分布)：

- Uniform Distribution(均匀分布)
- Normal Distribution/Gaussian Distribution(正态分布/高斯分布)
- Exponential Distribution(指数分布)
- Lognormal Distribution(对数正态分布)
- Gamma Distribution(Gamma分布)
- Beta Distribution(Beta分布)
- Dirichlet Distribution(狄利克雷分布)
- Rayleigh Distribution(瑞利分布)
- Cauchy Distribution(柯西分布)
- Weibull Distribution (韦伯分布)

> Three Sampling Distribution(三大抽样分布)：

- Chi-square Distribution(卡方分布)
- t-distribution(t-分布)
- F-distribution(F-分布)

<B> Data Pre-processing(数据预处理)</B>：

- Missing Value Imputation(缺失值填充)
- Discretization(离散化)
- Mapping(映射)
- Normalization(归一化/标准化)

<B> Sampling(采样)</B>：

- Simple Random Sampling(简单随机采样)
- Offline Sampling(离线等可能K采样)
- Online Sampling(在线等可能K采样)
- Ratio-based Sampling(等比例随机采样)
- Acceptance-rejection Sampling(接受-拒绝采样)
- Importance Sampling(重要性采样)
- MCMC(Markov Chain MonteCarlo 马尔科夫蒙特卡罗采样算法：Metropolis-Hasting& Gibbs)

<B> Clustering(聚类)</B>：

- K-MeansK-Mediods
- 二分K-Means
- FK-Means
- Canopy
- Spectral-KMeans(谱聚类)
- GMM-EM(混合高斯模型-期望最大化算法解决)
- K-Pototypes
- CLARANS(基于划分)
- BIRCH(基于层次)
- CURE(基于层次)
- STING(基于网格)
- CLIQUE(基于密度和基于网格)
- 2014年Science上的密度聚类算法等

> Clustering Effectiveness Evaluation(聚类效果评估)：

- Purity(纯度)
- RI(Rand Index, 芮氏指标)
- ARI(Adjusted Rand Index, 调整的芮氏指标)
- NMI(Normalized Mutual Information, 规范化互信息)
- F-meaure(F测量)

<B> Classification&Regression(分类&回归)</B>：

- LR(Linear Regression, 线性回归)
- LR(Logistic Regression, 逻辑回归)
- SR(Softmax Regression, 多分类逻辑回归)
- GLM(Generalized Linear Model, 广义线性模型)
- RR(Ridge Regression, 岭回归/L2正则最小二乘回归)，LASSO(Least Absolute Shrinkage and Selectionator Operator , L1正则最小二乘回归)
- DT(Decision Tree决策树)
- RF(Random Forest, 随机森林)
- GBDT(Gradient Boosting Decision Tree, 梯度下降决策树)
- CART(Classification And Regression Tree 分类回归树)
- KNN(K-Nearest Neighbor, K近邻)
- SVM(Support Vector Machine, 支持向量机, 包括SVC(分类)&SVR(回归))
- CBA(Classification based on Association Rule, 基于关联规则的分类)
- KF(Kernel Function, 核函数) 

	1. Polynomial Kernel Function(多项式核函数)
	1. Guassian Kernel Function(高斯核函数)
	1. Radial Basis Function(RBF径向基函数)
	1. String Kernel Function 字符串核函数


- NB(Naive Bayesian,朴素贝叶斯)
- BN(Bayesian Network/Bayesian Belief Network/Belief Network 贝叶斯网络/贝叶斯信度网络/信念网络)
- LDA(Linear Discriminant Analysis/Fisher Linear Discriminant 线性判别分析/Fisher线性判别)
- EL(Ensemble Learning, 集成学习) 

	1. Boosting
	1. Bagging
	1. Stacking
	1. AdaBoost(Adaptive Boosting 自适应增强)
- MEM(Maximum Entropy Model, 最大熵模型)

<B> Classification EffectivenessEvaluation(分类效果评估)</B>

- Confusion Matrix(混淆矩阵)
- Precision(精确度)
- Recall(召回率)
- Accuracy(准确率)
- F-score(F得分)
- ROC Curve(ROC曲线)
- AUC(AUC面积)
- Lift Curve(Lift曲线)
- KS Curve(KS曲线)

<B> PGM(Probabilistic Graphical Models, 概率图模型)</B>

- BN(BayesianNetwork/Bayesian Belief Network/ Belief Network , 贝叶斯网络/贝叶斯信度网络/信念网络)
- MC(Markov Chain, 马尔科夫链)
- MEM(Maximum Entropy Model, 最大熵模型)
- HMM(Hidden Markov Model, 马尔科夫模型)
- MEMM(Maximum Entropy Markov Model, 最大熵马尔科夫模型)
- CRF(Conditional Random Field,条件随机场)
- MRF(Markov Random Field, 马尔科夫随机场)
- Viterbi(维特比算法)

<B> NN(Neural Network, 神经网络)</B>

- ANN(Artificial Neural Network, 人工神经网络)
- SNN(Static Neural Network, 静态神经网络)
- BP(Error Back Propagation, 误差反向传播)
- HN(Hopfield Network)
- DNN(Dynamic Neural Network, 动态神经网络)
- RNN(Recurrent Neural Network, 循环神经网络)
- SRN(Simple Recurrent Network, 简单的循环神经网络)
- ESN(Echo State Network, 回声状态网络)
- LSTM(Long Short Term Memory, 长短记忆神经网络)
- CW-RNN(Clockwork-Recurrent Neural Network, 时钟驱动循环神经网络, 2014ICML）等.

<B> Deep Learning(深度学习)</B>

- Auto-encoder(自动编码器)
- SAE(Stacked Auto-encoders堆叠自动编码器) 

	1. Sparse Auto-encoders(稀疏自动编码器)
	1. Denoising Auto-encoders(去噪自动编码器)
	1. Contractive Auto-encoders(收缩自动编码器)
- RBM(Restricted Boltzmann Machine, 受限玻尔兹曼机)
- DBN(Deep Belief Network, 深度信念网络)
- CNN(Convolutional Neural Network, 卷积神经网络)
- Word2Vec(词向量学习模型)

<B> Dimensionality Reduction(降维)</B>

- LDA(Linear Discriminant Analysis/Fisher Linear Discriminant, 线性判别分析/Fish线性判别)
- PCA(Principal Component Analysis, 主成分分析)
- ICA(Independent Component Analysis, 独立成分分析)
- SVD(Singular Value Decomposition 奇异值分解)
- FA(Factor Analysis 因子分析法)

<B> Text Mining(文本挖掘)</B>

- VSM(Vector Space Model, 向量空间模型)
- Word2Vec(词向量学习模型)
- TF(Term Frequency, 词频)
- TF-IDF(TermFrequency-Inverse Document Frequency, 词频-逆向文档频率)
- MI(Mutual Information, 互信息)
- ECE(Expected Cross Entropy, 期望交叉熵)
- QEMI(二次信息熵)
- IG(Information Gain, 信息增益)
- IGR(Information Gain Ratio, 信息增益率)
- Gini(基尼系数)
- x2 Statistic(x2统计量)
- TEW(Text Evidence Weight, 文本证据权)
- OR(Odds Ratio, 优势率)
- N-Gram Model
- LSA(Latent Semantic Analysis, 潜在语义分析)
- PLSA(Probabilistic Latent Semantic Analysis, 基于概率的潜在语义分析)
- LDA(Latent Dirichlet Allocation, 潜在狄利克雷模型)
- SLM(Statistical Language Model, 统计语言模型)
- NPLM(Neural Probabilistic Language Model, 神经概率语言模型)
- CBOW(Continuous Bag of Words Model, 连续词袋模型)
- Skip-gram(Skip-gram Model)

<B> Association Mining(关联挖掘)</B>

Apriori算法
- FP-growth(Frequency Pattern Tree Growth, 频繁模式树生长算法)
- MSApriori(Multi Support-based Apriori, 基于多支持度的Apriori算法)
- GSpan(Graph-based Substructure Pattern Mining, 频繁子图挖掘)


<B> Sequential Patterns Analysis(序列模式分析)</B>

- AprioriAll
- Spade
- GSP(Generalized Sequential Patterns, 广义序列模式)
- PrefixSpan

<B> Forecast(预测)</B>

- LR(Linear Regression, 线性回归)
- SVR(Support Vector Regression, 支持向量机回归)
- ARIMA(Autoregressive Integrated Moving Average Model, 自回归积分滑动平均模型)
- GM(Gray Model, 灰色模型)
- BPNN(BP Neural Network, 反向传播神经网络)
- SRN(Simple Recurrent Network, 简单循环神经网络)
- LSTM(Long Short Term Memory, 长短记忆神经网络)
- CW-RNN(Clockwork Recurrent Neural Network, 时钟驱动循环神经网络)
- ……

<B> Linked Analysis(链接分析)</B>

- HITS(Hyperlink-Induced Topic Search, 基于超链接的主题检索算法)
- PageRank(网页排名)

<B> Recommendation Engine(推荐引擎)</B>

- SVD
- Slope One
- DBR(Demographic-based Recommendation, 基于人口统计学的推荐)
- CBR(Context-based Recommendation, 基于内容的推荐)
- CF(Collaborative Filtering, 协同过滤)
- UCF(User-based Collaborative Filtering Recommendation, 基于用户的协同过滤推荐)
- ICF(Item-based Collaborative Filtering Recommendation, 基于项目的协同过滤推荐)

<B> Similarity Measure&Distance Measure(相似性与距离度量)</B>

- EuclideanDistance(欧式距离)
- Chebyshev Distance(切比雪夫距离)
- Minkowski Distance(闵可夫斯基距离)
- Standardized EuclideanDistance(标准化欧氏距离)
- Mahalanobis Distance(马氏距离)
- Cos(Cosine, 余弦)
- Hamming Distance/Edit Distance(汉明距离/编辑距离)
- Jaccard Distance(杰卡德距离)
- Correlation Coefficient Distance(相关系数距离)
- Information Entropy(信息熵)
- KL(Kullback-Leibler Divergence, KL散度/Relative Entropy, 相对熵)

<B> Optimization(最优化)</B>

> Non-constrained Optimization(无约束优化)

- Cyclic Variable Methods(变量轮换法)
- Variable Simplex Methods(可变单纯形法)
- Newton Methods(牛顿法)
- Quasi-Newton Methods(拟牛顿法)
- Conjugate Gradient Methods(共轭梯度法)。

> Constrained Optimization(有约束优化)

- Approximation Programming Methods(近似规划法)
- Penalty Function Methods(罚函数法)
- Multiplier Methods(乘子法)。
- Heuristic Algorithm(启发式算法)
- SA(Simulated Annealing, 模拟退火算法)
- GA(Genetic Algorithm, 遗传算法)
- ACO(Ant Colony Optimization, 蚁群算法)

<B> Feature Selection(特征选择)</B>

- Mutual Information(互信息)
- Document Frequence(文档频率)
- Information Gain(信息增益)
- Chi-squared Test(卡方检验)
- Gini(基尼系数)


<B> Outlier Detection(异常点检测)</B>

- Statistic-based(基于统计)
- Density-based(基于密度)
- Clustering-based(基于聚类)。

<B> Tool(工具)</B>

- MPI
- Hadoop生态圈
- Spark
- IGraph
- BSP
- Weka
- Mahout
- Scikit-learn
- PyBrain
- Theano 



# <center> 附B Python与其他语言调用 </center>

> B1 C#调用Python脚本并使用Python的第三方模块，详见 [https://chaolongzhang.github.io/2015/dotnet-call-python/](https://chaolongzhang.github.io/2015/dotnet-call-python/)

	var engine = IronPython.Hosting.Python.CreateEngine();
	var scope = engine.CreateScope();
	var source = engine.CreateScriptSourceFromFile("hello.py");
	source.Execute(scope);
	var say_hello = scope.GetVariable<Func<object>>("say_hello");
	say_hello();
	var get_text = scope.GetVariable<Func<object>>("get_text");
	var text = get_text().ToString();
	Console.WriteLine(text);
	var add = scope.GetVariable<Func<object, object, object>>("add");
	var result1 = add(1, 2);
	Console.WriteLine(result1);
	var result2 = add("hello ", "world");
	Console.WriteLine(result2);

> B2 PHP调用Python脚本并使用Python的第三方模块，详见 [http://www.cnblogs.com/baiboy/p/check.html/)](http://www.cnblogs.com/baiboy/p/check.html)

	<?php
	    $name=mb_convert_encoding($_POST['projectname'], "GBK","UTF-8");
	    $sumb=mb_convert_encoding($_POST['projectsumb'], "GBK","UTF-8");
	    $program="D:/Users/Administrator/Anaconda3/python E:/pythonSource/CheckArticle/CheckRepeat/checkIndex.py
                 ".$name." ".$sumb.""; #注意使用绝对路径.$name."".$sumb
	    $output = nl2br(shell_exec($program));
	    // $program1="D:/Users/Administrator/Anaconda3/python ../CheckRepeat/test.py 
                     ".$name." ".$sumb.""; #注意使用绝对路径.$name."".$sumb
	    // $result1 = shell_exec($program1);
	    echo mb_convert_encoding ($output,"UTF-8", "GBK");
	    // if ($output!=null){
	    //     print_r(nl2br(file_get_contents('../CheckRepeat/database/DealCorpus/checkout.txt')));
	    // }
	?>

> B3 Python脚本调用JAVA并使用其第三方模块，详见 [http://www.cnblogs.com/baiboy/p/7676236.html/)](http://www.cnblogs.com/baiboy/p/7676236.html)

	# -*- coding:utf-8 -*-
	# Filename: main.py
	 
	from jpype import *
	 
	startJVM(getDefaultJVMPath(), "-Djava.class.path=C:\hanlp\hanlp-1.3.2.jar;C:\hanlp",
              "-Xms1g", "-Xmx1g") # 启动JVM，Linux需替换分号;为冒号:
	 
	print("="*30+"HanLP分词"+"="*30)
	HanLP = JClass('com.hankcs.hanlp.HanLP')
	# 中文分词
	print(HanLP.segment('你好，欢迎在Python中调用HanLP的API'))
	print("-"*70)



# <center> 附C Git项目上传简易教程 </center>

Linus在1991年创建了开源的Linux，从此，Linux系统不断发展，已经成为最大的服务器系统软件了。Linux的代码是如何管理的呢？Linus自己用C写了一个分布式版本控制系统，这就是Git！一个月之内，Linux系统的源码已经由Git管理了！Git迅速成为最流行的分布式版本控制系统，尤其是2008年，GitHub网站上线了，它为开源项目免费提供Git存储，无数开源项目开始迁移至GitHub，包括jQuery，PHP，Ruby等等。

(1) Windows版Git下载[https://git-scm.com/downloads](https://git-scm.com/downloads)，然后全部点击下一步安装。安装完成后，在开始菜单里找到Git->Git Bash说明Git安装成功。

(2) Git安装完成后，配置你的名字和Email地址。
<pre>
$ git config --global user.name "Your Name"
$ git config --global user.email "email@example.com"
</pre>

注意：git config命令的--global参数，表示本机所有的Git仓库都会使用这个配置，也可以对某个仓库指定不同的用户名和Email地址。

> 创建版本库

(1) 创建一个版本库非常简单，首先，创建一个空目录：

	$ cd /d //你指定的个人盘符
	 
	$ mkdir learngit //创建版本库根目录
	 
	$ cd learngit //进入版本库目录(tab键盘补全命令)
	 
	$ pwd //查看当前路径
	 
	/d/learngit

(2) 通过git init命令把这个目录变成Git可以管理的仓库：
	$ git init

(3) 在learngit下创建一个readme.txt文件并编写两句话。

	$touch readme.txt
	 
	$ vi readme.txt //进入编辑器，按i进入编辑模式，esc退出：wq强制保存
	 
	Git is a version control system.
	 
	Git is free software.
	 
	$cat readme.txt //查看信息

(4) 用命令git add告诉Git，把文件添加到仓库：

	$ git add readme.txt
(5) 用命令git commit告诉Git，把文件提交到仓库，-m后面输入的是本次提交的说明：

	$ git commit -m "wrote a readme file"

(6) 读者先自己注册个GitHub账号。然后即可查看上传文件的详细信息。


<B> 说明</B>

GitHub的是版本控制和协作代码托管平台，它可以让你和其他人的项目从任何地方合作。相对于CVS和SVN的联网限制和传速慢有明显的优势。因此，越来越受企业和个人的青睐。github上进行项目管理也是趋势。本文详细操作教程请访问个人主页：[http://www.cnblogs.com/baiboy/p/github.html#top](http://www.cnblogs.com/baiboy/p/github.html#top)


# <center> 参考文献 </center>

[1] 刘开瑛, 郭炳炎. 自然语言处理[M]. 科学出版社, 1991.

[2] 统计自然语言处理基础,Christopher.Manning等著,宛春法等译.

[3] 冯志伟. 现代语言学流派[M]. 陕西人民出版社, 1999.

[4] 冯志伟. 计算语言学基础[M]. 商务印书馆, 2001.

[5] 冯志伟. 数理语言学[M]. 知识出版社, 1985.

[6] 宗庆成, 吴华, 黄泰翼, 等. 限定领域汉语口语对话语料分析[J]. 计算语言学论文集》, 清华大学出版社, 1999.

[7] 李航. 统计学习方法[J]. 清华大学出版社, 北京, 2012.

[8] 数据挖掘概念与技术（364--386） 韩家炜

[9] 冯志伟. 自然语言处理简明教程[M]. 上海外语教育出版社, 2012.

[10] 冯志伟. 机器翻译今昔谈[M]. 北京：语文出版社，2007.

[11] 冯志伟.当前自然语言处理发展的四个特点[J].暨南大学华文学院学报. 2006(1).

[12] 吴军. 数学之美[M]. 人民邮电出版社, 2012.

[13] 白宁超, 唐聃, 王亚强. 基于主动学习的传统中医症状本体构建方法研究综述[J]. 电子技术与软件工程, 2016 (7): 162-163.
 
[14] 舒红平, 游志胜, 蒋建民. 基于信息熵的决策属性分类挖掘算法及应用[D].

[15] Russell, Matthew A. Mining the Social Web: Data Mining Facebook, Twitter, LinkedIn, Google+, GitHub, and More. O’Reilly Media, Inc. 2013.

[16] Csardi, Gabor. “Eigenvalues and eigenvectors of the adjacency matrix of a graph.”.

[17] Barthélemy, M. “Betweenness centrality in large complex networks.” Physics 38.2(2004):163-168.

[18] Russell, Matthew A. Mining the Social Web: Data Mining Facebook, Twitter, LinkedIn, Google+, GitHub, and More. O’Reilly Media, Inc. 2013.

[19] 王思力，张华平，王斌.双数组Trie树算法优化及其应用研究[J].中文信息学报，2006，20(5):24—30 

[20] 梁锐, 朱清新, 廖淑娇, 等. 基于多特征融合的深度视频自然语言描述方法[J]. 计算机应用, 2017, 37(4): 1179-1184.

[21] Souter C. Natural language identification using corpus-based models[J]. HERMES-Journal of Language and Communication in Business, 2017, 7(13): 183-203.

[22] Botha J A, Pitler E, Ma J, et al. Natural language processing with small feed-forward networks[J]. arXiv preprint arXiv:1708.00214, 2017.

[23] Sarfjoo S S, Demiroglu C, King S, et al. Using eigenvoices and nearest-neighbors in HMM-based cross-lingual speaker adaptation with limited data[J]. IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), 2017, 25(4): 839-851.

[24] Corbett-Detig R, Nielsen R. A hidden Markov model approach for simultaneously estimating local ancestry and admixture time using next generation sequence data in samples of arbitrary ploidy[J]. PLoS genetics, 2017, 13(1): e1006529.

[25] Mohamed S, Mahmoud T, Ibrahim M. Efficient Edge Detection Technique Based on Hidden Markov Model using Canny Operator[J]. threshold, 2017, 6(01).

[26] Jablonowski K. Hidden Markov Models for Protein Domain Homology Identification and Analysis[J]. SH2 Domains: Methods and Protocols, 2017: 47-58.

[27] Lapuyade-Lahorgue J, Xue J H, Ruan S. Segmenting Multi-Source images using hidden Markov fields with copula-based multivariate statistical distributions[J]. IEEE Transactions on Image Processing, 2017.

[28] Fiecas M, Franke J, von Sachs R, et al. Shrinkage estimation for multivariate hidden Markov models[J]. Journal of the American Statistical Association, 2017, 112(517): 424-435.

[29] DeRuiter S L, Langrock R, Skirbutas T, et al. A multivariate mixed hidden Markov model for blue whale behaviour and responses to sound exposure[J]. The Annals of Applied Statistics, 2017, 11(1): 362-392.

[30] Cowen L L E, Besbeas P, Morgan B J T, et al. Hidden Markov models for extended batch data[J]. Biometrics, 2017.

[31] Gu W, Lv Z, Hao M. Change detection method for remote sensing images based on an improved Markov random field[J]. Multimedia Tools and Applications, 2017, 76(17): 17719-17734.

[32] Messing R O, Pomrenze M B, De Guglielmo G, et al. A distributed CRF network in rat extended amygdala regulates anxiety and excessive alcohol drinking[J]. Alcohol, 2017, 60: 212.

[33] Miczek K A. CRF modulation in VTA-DRN escalates alcohol intake after social stress[J]. Alcohol, 2017, 60: 214-215.

[34] 邬伦, 刘磊, 李浩然, 等. 基于条件随机场的中文地名识别方法[J]. 武汉大学学报· 信息科学版, 2017, 42(2): 150-156.

[35] 万业号, 刘利军, 黄青松. 基于层叠条件随机场的中文医疗机构名识别[J]. 济南大学学报: 自然科学版, 2017 (1): 61-66.

[36] 李志义, 王冕, 赵鹏武. 基于条件随机场模型的 “评价特征-评价词” 对抽取研究[J]. 情报学报, 2017, 36(4): 411-421.

[37] 黄念娥, 黄河, 王儒敬. 本体与条件随机场结合的涉农商品名称抽取与类别标注[J]. 计算机应用, 2017, 37(1): 233-238.

[38] 翟菊叶, 陈春燕, 张钰, 等. 基于 CRF 与规则相结合的中文电子病历命名实体识别研究[J]. 包头医学院学报, 2017, 11: 052.

[39] 吴俊峰, 姜志国, 张浩鹏, 等. 半监督条件随机场的高光谱遥感图像分类[J]. 2017.

[40] 王茵, 周学广, 陆健. 基于条件随机场的中文情感分析方法比较研究[J]. 计算机与数字工程, 2017, 45(9): 1703-1707.

[41] 丁洁, 赵景惠. 基于 N—gram 模型的中文分词算法的研究[J]. 福建电脑, 2017, 33(5): 110-110.

[42] 邓丽萍, 罗智勇. 基于半监督 CRF 的跨领域中文分词[J]. 中文信息学报, 2017, 31(4): 9-19.

[43] 李雪莲, 段鸿, 许牧. 基于 GRU 神经网络的中文分词法[J]. 2017.

[44] 周俊, 郑中华, 张炜. 基于改进最大匹配算法的中文分词粗分方法[J]. 计算机工程与应用, 2014 (2): 124-128.

[45] 梁喜涛, 顾磊. 中文分词与词性标注研究[J]. 计算机技术与发展, 2015 (2015 年 02): 175-180.

[46] 杜丽萍, 李晓戈, 于根, 等. 基于互信息改进算法的新词发现对中文分词系统改进[J]. 北京大学学报 (自然科学版), 2016, 52(1): 35-40.

[47] Natural Language Toolkit.

[48] 赵铁军. 将语言计算的理论方法和最新成果呈现给读者-《 统计自然语言处理 》 评述[J]. 自动化学报, 2014, 40(5): 1024-1024.

[49] 李泽魁, 赵妍妍, 秦兵, 等. 中文微博情感倾向性分析特征工程[J]. 山西大学学报: 自然科学版, 2014, 37(4): 570-579.

[50] 秦兵, 刘安安, 刘挺. 无指导的中文开放式实体关系抽取[J]. 计算机研究与发展, 2015, 52(5): 1029-1035.

[51] 刘雄, 张宇, 张伟男, 等. 基于依存句法分析的复合事实型问句分解方法[J]. 中文信息学报, 2017, 31(3): 140-146.

[52] 刘明杨, 秦兵, 刘挺. 基于文采特征的高考作文自动评分[J]. 智能计算机与应用, 2016, 1: 001.

[53] 王巍, 赵铁军, 辛国栋, 等. 基于条件随机域模型的比较要素抽取研究[J]. 自动化学报, 2015, 41(8): 1385-1393.

[54] 赵宇婧, 许鑫泽, 朱齐丹, 等. 一种自然语言关键词的人机交互方法[J]. 2016.

[55] 王哲, 严璘璘, 孙宇浩, 等. 功能短句的语义成分对两可自然物体分类的影响[J]. 科学通报, 2017, 5: 007.

[56] Chen Y, Zhou G, He S, et al. The CASIA Entity linking System at TAC 2013[J]. 2014.

[57] 徐浩广, 王宁, 刘佳明, 等. 基于自然语言检索的综合相似度计算算法[J]. 计算机系统应用, 2017, 26(6): 170-175.

[58] Ling Z, Chunxiang X, Chengzhi Z, et al. User Tags and Microblog Posts: Case Study of Sina Weibo[J]. Data Analysis and Knowledge Discovery, 2016, 32(3): 18-24.

[59] 邓宇昂. 计算机语言发展探析[J]. 电子世界, 2017 (14): 80-80.

[60] 郝丹. 结合 NLP 技术的汉语学习系统设计与实现[D]. 华中师范大学, 2015.

[61] 曾文, 张均胜, 徐红姣, 等. 多语言科技语料库建设研究[J]. 数字图书馆论坛, 2015 (8): 43-47.

[62] 肖忠华. 英汉翻译中的汉语译文语料库研究[J]. 当代外语研究, 2016, 1: 001.

[63] 王克非, 秦洪武. 论平行语料库在翻译教学中的应用[J]. 外语教学与研究, 2015, 47(5): 763-772.

[64] Aijmer K, Altenberg B. English corpus linguistics[M]. Routledge, 2014.

[65] Kennedy G. An introduction to corpus linguistics[M]. Routledge, 2014.

[66] Chen W, Chen N F, Lim B P, et al. Corpus-based pronunciation variation rule analysis for singapore English[C]//SLaTE. 2015: 35-40.

[67] Laviosa S. The English Comparable Corpus[J]. Unity in diversity: Current trends in Translation Studies, 2016: 101.

[68] Talbert R. Inverting the linear algebra classroom[J]. Primus, 2014, 24(5): 361-374.

[69] Eddelbuettel D, Sanderson C. RcppArmadillo: Accelerating R with high-performance C++ linear algebra[J]. Computational Statistics & Data Analysis, 2014, 71: 1054-1063.

[70] Williams G. Linear algebra with applications[M]. Jones & Bartlett Learning, 2017.

[71] Pugachev V S. Probability theory and mathematical statistics for engineers[M]. Elsevier, 2014.

[72] Feller W. On the Central Limit Theorem of Probability Theory[M]//Selected Papers I. Springer International Publishing, 2015: 207-244.

[73] Durlauf S N. Lecture Notes 1: Probability Theory[J]. 2015.

[74] Cressie N. Statistics for spatial data[M]. John Wiley & Sons, 2015.

[75] Gravetter F J, Wallnau L B. Statistics for the behavioral sciences[M]. Cengage Learning, 2016.

[76] 谢庆华, 张宁蓉, 宋以胜, 等. 聚类数据挖掘可视化模型方法与技术[J]. 解放军理工大学学报: 自然科学版, 2015, 16(1): 7-15.

[77] 马昱欣, 曹震东, 陈为. 可视化驱动的交互式数据挖掘方法综述[J]. 计算机辅助设计与图形学学报, 2016, 28(1): 1-8.

[78] 杨彦波, 刘滨, 祁明月. 信息可视化研究综述[J]. 河北科技大學學報, 2014, 35(1): 91-102.

[79] 王子毅, 张春海. 基于 ECharts 的数据可视化分析组件设计实现[J]. 微型机与应用, 2016, 35(14): 46-48.

[80] 姚霖, 刘轶, 李鑫鑫, 等. 词边界字向量的中文命名实体识别[J]. 智能系统学报, 2016 (2016 年 01): 37-42.

[81] 维基百科.

[82] 博客园-伏草惟存

[83] GitHub

[84] Sublime官网

[85] Anaconda官网

[86] 维基百科

[87] NLTK官方网站

[88] Beautiful Soup 4.2.0 文档.

[89] 11款开放中文分词引擎评测.

[90] 百度Echart官网












